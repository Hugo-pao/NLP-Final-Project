{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "The final project is designed to let you apply what you have learned so far, and demonstrate that you have mastered it. The submission will be graded on the correctness and performance of the execution of your analysis (50%), the ambitiousness of the problems chosen (30%), and the creativity of your questions and solutions (20%).\n",
    "\n",
    "Your submission should include all outputs and be *self-contained*, so it can be executed if necessary.\n",
    "\n",
    "The submission includes two parts:\n",
    "1. this notebook\n",
    "2. a 15-min presentation, to be held on May 8\n",
    "\n",
    "\n",
    "## Submission\n",
    "The project is due on ***May 07, 23:59 CET*** (counted as the time stamp when it is received). Late submissions will **not** be considered, and graded as 0! \n",
    "\n",
    "To submit, please:\n",
    "\n",
    "1. copy this file and all additional data into a folder with your group ID\n",
    "3. zip the folder\n",
    "4. send a copy of the zip file to Dirk Hovy <dirk.hovy@unibocconi.it> and Tommaso Fornaciari <fornaciari@unibocconi.it>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data, Preprocessing, and Annotation (4 pts)\n",
    "\n",
    "Find a data set for text classification and a data set for structured prediction. These can be the same.\n",
    "Kaggle is a good place to start, or the Google data set search. \n",
    "\n",
    "The data sets should have **at least 5,000** documents each. **At least 2000 instances** need to be labeled. If there is no label provided, you can annotate your own. You can get up to **3 bonus points** for annotation, depending on the amount and complexity of the annotation.\n",
    "\n",
    "Split the data into dedicated training, development, and test sets (if they do not include these already)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly (max. 100 words!) describe the content and type of the data set, and what you are planning to look at. \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data and explain (max. 200 words) which preprocessing steps you chose and why, and give statistics of the number of documents, types, and tokens, before and after preprocessing.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.theonion.com/thirtysomething-scien...   \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...   \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...   \n",
       "3  https://local.theonion.com/inclement-weather-p...   \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  thirtysomething scientists unveil doomsday clo...             1  \n",
       "1  dem rep. totally nails why congress is falling...             0  \n",
       "2  eat your veggies: 9 deliciously different recipes             0  \n",
       "3  inclement weather prevents liar from getting t...             1  \n",
       "4  mother comes pretty close to using word 'strea...             1  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "\n",
    "data = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "def clean(text):\n",
    "    return ' '.join([token.lemma_ \n",
    "             for token in nlp(text) \n",
    "             if token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV', 'X'}])\n",
    "\n",
    "def verb(text):\n",
    "    '''\n",
    "    return the strings with the verbs only\n",
    "    '''\n",
    "    return ' '.join([token.lemma_ \n",
    "             for token in nlp(text) \n",
    "             if token.pos_ in {'VERB'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned'] = data.headline.apply(clean)\n",
    "data['verb'] = data.headline.apply(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 28619 article headlines\n",
      "Presence of NAN False\n",
      "0    14985\n",
      "1    13634\n",
      "Name: is_sarcastic, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fcb8bcdeda0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFkpJREFUeJzt3W+QXfV93/H3JyjYWLENNvWWkWhFGsUNhnZCdjBpZtKNSUGQDOKB3YEhRbiaasYhbprQJnL9gI4dZuymlAbqP1WMKvBQY0KSShNwiAZzx23HYLCxEX9C2WAKa4ixK6CWqe3I+fbB/cm91llpV/eu7tWu3q+ZHZ3zPb9zz++7iP3s+XOvUlVIkjToRyY9AUnSscdwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKlj1aQnMKxTTz211q1bN9S+3/72t1m9evXSTugYZ8/Hh+Ot5+OtXxi95y9+8YvfrKq/sdC4ZRsO69at46GHHhpq316vx8zMzNJO6Bhnz8eH463n461fGL3nJP9rMeO8rCRJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSepYtu+QHsWer73CVVvvGvtxn/nQL439mJI0DM8cJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHQuGQ5LtSV5M8ug82/5lkkpyaltPkhuTzCZ5JMk5A2M3JXmqfW0aqP9Mkj1tnxuTZKmakyQNZzFnDjuADQcXk5wO/CPg2YHyRcD69rUF+Fgb+ybgWuDtwLnAtUlOaft8rI09sF/nWJKk8VowHKrqc8DeeTbdAPwWUAO1jcCt1Xc/cHKS04ALgd1VtbeqXgJ2AxvatjdU1eerqoBbgUtHa0mSNKqhPngvySXA16rqKwddBVoDPDewPtdqh6vPzVM/1HG30D/LYGpqil6vN8z0mToJrjl7/1D7jmLY+S6Fffv2TfT4k2DPK9/x1i+Mr+cjDockrwPeD1ww3+Z5ajVEfV5VtQ3YBjA9PV0zMzMLTXdeN922k+v3jP8DaZ+5Ymbsxzyg1+sx7PdrubLnle946xfG1/MwTyv9HeAM4CtJngHWAl9K8jfp/+Z/+sDYtcDzC9TXzlOXJE3QEYdDVe2pqrdU1bqqWkf/B/w5VfWXwC7gyvbU0nnAK1X1AnAPcEGSU9qN6AuAe9q2byU5rz2ldCWwc4l6kyQNaTGPsn4K+Dzw1iRzSTYfZvjdwNPALPD7wK8CVNVe4IPAg+3rA60G8B7gE22fvwA+M1wrkqSlsuCF96q6fIHt6waWC7j6EOO2A9vnqT8EnLXQPCTpWLJuAv+aJMCODavHchzfIS1J6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2L+Tektyd5McmjA7XfTfLnSR5J8sdJTh7Y9r4ks0meTHLhQH1Dq80m2TpQPyPJA0meSvLpJCcuZYOSpCO3mDOHHcCGg2q7gbOq6u8B/xN4H0CSM4HLgLe1fT6a5IQkJwAfAS4CzgQub2MBPgzcUFXrgZeAzSN1JEka2YLhUFWfA/YeVPuzqtrfVu8H1rbljcDtVfXdqvoqMAuc275mq+rpqvoecDuwMUmAdwB3tv1vAS4dsSdJ0oiW4p7DPwU+05bXAM8NbJtrtUPV3wy8PBA0B+qSpAlaNcrOSd4P7AduO1CaZ1gxfwjVYcYf6nhbgC0AU1NT9Hq9I5nuD0ydBNecvX/hgUts2PkuhX379k30+JNgzyvfJPudxM8QGF/PQ4dDkk3ALwPnV9WBH+hzwOkDw9YCz7fl+erfBE5OsqqdPQyO76iqbcA2gOnp6ZqZmRlq7jfdtpPr94yUi0N55oqZsR/zgF6vx7Dfr+XKnle+SfZ71da7JnLcHRtWj6XnoS4rJdkA/DZwSVW9OrBpF3BZktckOQNYD3wBeBBY355MOpH+TetdLVTuA97Z9t8E7ByuFUnSUlnMo6yfAj4PvDXJXJLNwH8EXg/sTvLlJB8HqKrHgDuAx4E/Ba6uqu+3s4JfA+4BngDuaGOhHzK/mWSW/j2Im5e0Q0nSEVvw2kpVXT5P+ZA/wKvqOuC6eep3A3fPU3+a/tNMkqRjhO+QliR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktSxYDgk2Z7kxSSPDtTelGR3kqfan6e0epLcmGQ2ySNJzhnYZ1Mb/1SSTQP1n0myp+1zY5IsdZOSpCOzmDOHHcCGg2pbgXuraj1wb1sHuAhY3762AB+DfpgA1wJvB84Frj0QKG3MloH9Dj6WJGnMFgyHqvocsPeg8kbglrZ8C3DpQP3W6rsfODnJacCFwO6q2ltVLwG7gQ1t2xuq6vNVVcCtA68lSZqQYe85TFXVCwDtz7e0+hrguYFxc612uPrcPHVJ0gStWuLXm+9+QQ1Rn//Fky30L0ExNTVFr9cbYoowdRJcc/b+ofYdxbDzXQr79u2b6PEnwZ5Xvkn2O4mfITC+nocNh68nOa2qXmiXhl5s9Tng9IFxa4HnW33moHqv1dfOM35eVbUN2AYwPT1dMzMzhxp6WDfdtpPr9yx1Li7smStmxn7MA3q9HsN+v5Yre175JtnvVVvvmshxd2xYPZaeh72stAs48MTRJmDnQP3K9tTSecAr7bLTPcAFSU5pN6IvAO5p276V5Lz2lNKVA68lSZqQBX99TvIp+r/1n5pkjv5TRx8C7kiyGXgWeFcbfjdwMTALvAq8G6Cq9ib5IPBgG/eBqjpwk/s99J+IOgn4TPuSJE3QguFQVZcfYtP584wt4OpDvM52YPs89YeAsxaahyRpfHyHtCSpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqSOkcIhyW8keSzJo0k+leS1Sc5I8kCSp5J8OsmJbexr2vps275u4HXe1+pPJrlwtJYkSaMaOhySrAH+OTBdVWcBJwCXAR8Gbqiq9cBLwOa2y2bgpar6CeCGNo4kZ7b93gZsAD6a5IRh5yVJGt2ol5VWASclWQW8DngBeAdwZ9t+C3BpW97Y1mnbz0+SVr+9qr5bVV8FZoFzR5yXJGkEq4bdsaq+luTfAc8C/xf4M+CLwMtVtb8NmwPWtOU1wHNt3/1JXgHe3Or3D7z04D4/JMkWYAvA1NQUvV5vqLlPnQTXnL1/4YFLbNj5LoV9+/ZN9PiTYM8r3yT7ncTPEBhfz0OHQ5JT6P/WfwbwMvAHwEXzDK0Duxxi26Hq3WLVNmAbwPT0dM3MzBzZpJubbtvJ9XuGbn1oz1wxM/ZjHtDr9Rj2+7Vc2fPKN8l+r9p610SOu2PD6rH0PMplpV8EvlpV36iqvwL+CPgHwMntMhPAWuD5tjwHnA7Qtr8R2DtYn2cfSdIEjBIOzwLnJXldu3dwPvA4cB/wzjZmE7CzLe9q67Ttn62qavXL2tNMZwDrgS+MMC9J0ohGuefwQJI7gS8B+4GH6V/yuQu4PcnvtNrNbZebgU8mmaV/xnBZe53HktxBP1j2A1dX1feHnZckaXQjXXivqmuBaw8qP808TxtV1XeAdx3ida4DrhtlLpKkpeM7pCVJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqWOkcEhycpI7k/x5kieS/GySNyXZneSp9ucpbWyS3JhkNskjSc4ZeJ1NbfxTSTaN2pQkaTSjnjn8HvCnVfV3gb8PPAFsBe6tqvXAvW0d4CJgffvaAnwMIMmb6P871G+n/29PX3sgUCRJkzF0OCR5A/DzwM0AVfW9qnoZ2Ajc0obdAlzaljcCt1bf/cDJSU4DLgR2V9XeqnoJ2A1sGHZekqTRjXLm8OPAN4D/nOThJJ9IshqYqqoXANqfb2nj1wDPDew/12qHqkuSJmTViPueA7y3qh5I8nv8/0tI88k8tTpMvfsCyRb6l6SYmpqi1+sd0YQPmDoJrjl7/1D7jmLY+S6Fffv2TfT4k2DPK98k+53EzxAYX8+jhMMcMFdVD7T1O+mHw9eTnFZVL7TLRi8OjD99YP+1wPOtPnNQvTffAatqG7ANYHp6umZmZuYbtqCbbtvJ9XtGaX04z1wxM/ZjHtDr9Rj2+7Vc2fPKN8l+r9p610SOu2PD6rH0PPRlpar6S+C5JG9tpfOBx4FdwIEnjjYBO9vyLuDK9tTSecAr7bLTPcAFSU5pN6IvaDVJ0oSM+uvze4HbkpwIPA28m37g3JFkM/As8K429m7gYmAWeLWNpar2Jvkg8GAb94Gq2jvivCRJIxgpHKrqy8D0PJvOn2dsAVcf4nW2A9tHmYskaen4DmlJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHSOHQ5ITkjyc5E/a+hlJHkjyVJJPJzmx1V/T1mfb9nUDr/G+Vn8yyYWjzkmSNJqlOHP4deCJgfUPAzdU1XrgJWBzq28GXqqqnwBuaONIciZwGfA2YAPw0SQnLMG8JElDGikckqwFfgn4RFsP8A7gzjbkFuDStryxrdO2n9/GbwRur6rvVtVXgVng3FHmJUkazahnDv8B+C3gr9v6m4GXq2p/W58D1rTlNcBzAG37K238D+rz7CNJmoBVw+6Y5JeBF6vqi0lmDpTnGVoLbDvcPgcfcwuwBWBqaoper3ckU/6BqZPgmrP3LzxwiQ0736Wwb9++iR5/Eux55Ztkv5P4GQLj63nocAB+DrgkycXAa4E30D+TODnJqnZ2sBZ4vo2fA04H5pKsAt4I7B2oHzC4zw+pqm3ANoDp6emamZkZauI33baT6/eM0vpwnrliZuzHPKDX6zHs92u5sueVb5L9XrX1rokcd8eG1WPpeejLSlX1vqpaW1Xr6N9Q/mxVXQHcB7yzDdsE7GzLu9o6bftnq6pa/bL2NNMZwHrgC8POS5I0uqPx6/NvA7cn+R3gYeDmVr8Z+GSSWfpnDJcBVNVjSe4AHgf2A1dX1fePwrwkSYu0JOFQVT2g15afZp6njarqO8C7DrH/dcB1SzEXSdLofIe0JKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpI6hwyHJ6UnuS/JEkseS/HqrvynJ7iRPtT9PafUkuTHJbJJHkpwz8Fqb2vinkmwavS1J0ihGOXPYD1xTVT8FnAdcneRMYCtwb1WtB+5t6wAXAevb1xbgY9APE+Ba4O3AucC1BwJFkjQZQ4dDVb1QVV9qy98CngDWABuBW9qwW4BL2/JG4Nbqux84OclpwIXA7qraW1UvAbuBDcPOS5I0ulVL8SJJ1gE/DTwATFXVC9APkCRvacPWAM8N7DbXaoeqz3ecLfTPOpiamqLX6w0136mT4Jqz9w+17yiGne9S2Ldv30SPPwn2vPJNst9J/AyB8fU8cjgk+THgD4F/UVX/J8khh85Tq8PUu8WqbcA2gOnp6ZqZmTni+QLcdNtOrt+zJLl4RJ65Ymbsxzyg1+sx7PdrubLnlW+S/V619a6JHHfHhtVj6Xmkp5WS/Cj9YLitqv6olb/eLhfR/nyx1eeA0wd2Xws8f5i6JGlCRnlaKcDNwBNV9e8HNu0CDjxxtAnYOVC/sj21dB7wSrv8dA9wQZJT2o3oC1pNkjQho1xb+TngnwB7kny51f418CHgjiSbgWeBd7VtdwMXA7PAq8C7Aapqb5IPAg+2cR+oqr0jzEuSNKKhw6Gq/jvz3y8AOH+e8QVcfYjX2g5sH3YukqSl5TukJUkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySp45gJhyQbkjyZZDbJ1knPR5KOZ8dEOCQ5AfgIcBFwJnB5kjMnOytJOn4dE+EAnAvMVtXTVfU94HZg44TnJEnHrWMlHNYAzw2sz7WaJGkCVk16Ak3mqVVnULIF2NJW9yV5csjjnQp8c8h9h5YPj/uIP2QiPU+YPa98x1u//MKHR+75by9m0LESDnPA6QPra4HnDx5UVduAbaMeLMlDVTU96ussJ/Z8fDjeej7e+oXx9XysXFZ6EFif5IwkJwKXAbsmPCdJOm4dE2cOVbU/ya8B9wAnANur6rEJT0uSjlvHRDgAVNXdwN1jOtzIl6aWIXs+PhxvPR9v/cKYek5V576vJOk4d6zcc5AkHUNWdDgs9JEcSV6T5NNt+wNJ1o1/lktnEf3+ZpLHkzyS5N4ki3qk7Vi22I9dSfLOJJVk2T/Zspiek/zj9t/6sST/ZdxzXGqL+Lv9t5Lcl+Th9vf74knMc6kk2Z7kxSSPHmJ7ktzYvh+PJDlnySdRVSvyi/6N7b8Afhw4EfgKcOZBY34V+Hhbvgz49KTnfZT7/QXgdW35Pcu538X23Ma9HvgccD8wPel5j+G/83rgYeCUtv6WSc97DD1vA97Tls8Enpn0vEfs+eeBc4BHD7H9YuAz9N8jdh7wwFLPYSWfOSzmIzk2Are05TuB85PM94a85WDBfqvqvqp6ta3eT//9JMvZYj925YPAvwW+M87JHSWL6fmfAR+pqpcAqurFMc9xqS2m5wLe0JbfyDzvk1pOqupzwN7DDNkI3Fp99wMnJzltKeewksNhMR/J8YMxVbUfeAV481hmt/SO9CNINtP/zWM5W7DnJD8NnF5VfzLOiR1Fi/nv/JPATyb5H0nuT7JhbLM7OhbT878BfiXJHP2nHt87nqlNzFH/yKFj5lHWo2AxH8mxqI/tWCYW3UuSXwGmgX94VGd09B225yQ/AtwAXDWuCY3BYv47r6J/aWmG/tnhf0tyVlW9fJTndrQspufLgR1VdX2SnwU+2Xr+66M/vYk46j+7VvKZw2I+kuMHY5Kson86erhTuWPZoj6CJMkvAu8HLqmq745pbkfLQj2/HjgL6CV5hv612V3L/Kb0Yv9e76yqv6qqrwJP0g+L5WoxPW8G7gCoqs8Dr6X/uUsr1aL+fx/FSg6HxXwkxy5gU1t+J/DZand7lqEF+22XWP4T/WBY7tehYYGeq+qVqjq1qtZV1Tr691kuqaqHJjPdJbGYv9f/lf7DByQ5lf5lpqfHOsultZienwXOB0jyU/TD4RtjneV47QKubE8tnQe8UlUvLOUBVuxlpTrER3Ik+QDwUFXtAm6mf/o5S/+M4bLJzXg0i+z3d4EfA/6g3Xd/tqoumdikR7TInleURfZ8D3BBkseB7wP/qqr+9+RmPZpF9nwN8PtJfoP+5ZWrlvEveiT5FP3Lgqe2+yjXAj8KUFUfp39f5WJgFngVePeSz2EZf/8kSUfJSr6sJEkakuEgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6/h9Wk3QpNQ0jOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.drop(\"article_link\", axis = 1, inplace = True)\n",
    "\n",
    "data['Set'] = np.random.choice(\n",
    "    ['train', 'valid', 'test'],\n",
    "    data.shape[0],\n",
    "    p=[0.7, 0.15, 0.15]\n",
    ")\n",
    "\n",
    "print(\"There are\", data.shape[0],'article headlines')\n",
    "print(\"Presence of NAN\" , data.isna().values.any())\n",
    "print(data.is_sarcastic.value_counts())\n",
    "data.is_sarcastic.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>verb</th>\n",
       "      <th>Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientist hair loss</td>\n",
       "      <td>NaN</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>0</td>\n",
       "      <td>totally nail why fall short gender racial equa...</td>\n",
       "      <td>nail fall</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>0</td>\n",
       "      <td>eat veggie deliciously different recipe</td>\n",
       "      <td>eat</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevent liar get work</td>\n",
       "      <td>prevent get work</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>1</td>\n",
       "      <td>mother come pretty close use word streaming co...</td>\n",
       "      <td>come use</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           headline  \\\n",
       "0           0  thirtysomething scientists unveil doomsday clo...   \n",
       "1           1  dem rep. totally nails why congress is falling...   \n",
       "2           2  eat your veggies: 9 deliciously different recipes   \n",
       "3           3  inclement weather prevents liar from getting t...   \n",
       "4           4  mother comes pretty close to using word 'strea...   \n",
       "\n",
       "   is_sarcastic                                            cleaned  \\\n",
       "0             1                thirtysomething scientist hair loss   \n",
       "1             0  totally nail why fall short gender racial equa...   \n",
       "2             0            eat veggie deliciously different recipe   \n",
       "3             1            inclement weather prevent liar get work   \n",
       "4             1  mother come pretty close use word streaming co...   \n",
       "\n",
       "               verb    Set  \n",
       "0               NaN  valid  \n",
       "1         nail fall  train  \n",
       "2               eat  train  \n",
       "3  prevent get work  train  \n",
       "4          come use  train  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcasm = data.loc[data[\"is_sarcastic\"] == 1,[\"headline\",\"cleaned\",\"verb\"]]\n",
    "normal = data.loc[data[\"is_sarcastic\"] == 0,[\"headline\",\"cleaned\",\"verb\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research Question(s) (2 pts)\n",
    "\n",
    "Describe what question you are investigating with the data (max. 100 words)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis (6 pts)\n",
    "\n",
    "Apply at least one version of *each* of the following analysis methods to the data set (justify your choices):\n",
    "1. Topic modeling with LDA (3 pts). Justify your choice of number of topics!\n",
    "2. Dense word embeddings ***or*** document embeddings: visualize these and show a clustering (3 pts) \n",
    "\n",
    "Your analysis needs to be run on the training data only! You can use the dev set for tuning.\n",
    "\n",
    "### Other methods\n",
    "If appropriate for your problem, feel free to explore other methods, as long as they do not require additional libraries (**up to 2 bonus points**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcasm Words\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>right</td>\n",
       "      <td>136</td>\n",
       "      <td>5.597771</td>\n",
       "      <td>110.407686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>thing</td>\n",
       "      <td>147</td>\n",
       "      <td>5.547938</td>\n",
       "      <td>111.974877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>die</td>\n",
       "      <td>145</td>\n",
       "      <td>5.534145</td>\n",
       "      <td>117.808351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>parent</td>\n",
       "      <td>148</td>\n",
       "      <td>5.527319</td>\n",
       "      <td>119.167663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>little</td>\n",
       "      <td>144</td>\n",
       "      <td>5.541018</td>\n",
       "      <td>119.414129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word   tf       idf       tfidf\n",
       "26   right  136  5.597771  110.407686\n",
       "32   thing  147  5.547938  111.974877\n",
       "6      die  145  5.534145  117.808351\n",
       "22  parent  148  5.527319  119.167663\n",
       "14  little  144  5.541018  119.414129"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                             min_df=0.01, #df stands for document frequency\n",
    "                             max_df=0.6, \n",
    "                             stop_words='english')\n",
    "X = tfidf_vectorizer.fit_transform(sarcasm.cleaned.dropna().tolist())\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', min_df=0.01, max_df=0.6, stop_words='english')\n",
    "\n",
    "X2 = vectorizer.fit_transform(sarcasm.cleaned.dropna().tolist())\n",
    "\n",
    "df = pd.DataFrame(data={'word': vectorizer.get_feature_names(), \n",
    "                        'tf': X2.sum(axis=0).A1, \n",
    "                        'idf': tfidf_vectorizer.idf_,\n",
    "                        'tfidf': X.sum(axis=0).A1\n",
    "                       })\n",
    "print(\"Sarcasm Words\")\n",
    "df = df.sort_values(['tfidf', 'tf', 'idf'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>police</td>\n",
       "      <td>157</td>\n",
       "      <td>5.557985</td>\n",
       "      <td>132.332740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>win</td>\n",
       "      <td>155</td>\n",
       "      <td>5.583960</td>\n",
       "      <td>134.050935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>old</td>\n",
       "      <td>160</td>\n",
       "      <td>5.545245</td>\n",
       "      <td>134.725503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>health</td>\n",
       "      <td>153</td>\n",
       "      <td>5.577403</td>\n",
       "      <td>135.023636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>big</td>\n",
       "      <td>161</td>\n",
       "      <td>5.538936</td>\n",
       "      <td>138.642797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word   tf       idf       tfidf\n",
       "20  police  157  5.557985  132.332740\n",
       "31     win  155  5.583960  134.050935\n",
       "18     old  160  5.545245  134.725503\n",
       "6   health  153  5.577403  135.023636\n",
       "1      big  161  5.538936  138.642797"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                             min_df=0.01, \n",
    "                             max_df=0.6, \n",
    "                             stop_words='english')\n",
    "X = tfidf_vectorizer.fit_transform(normal.cleaned.dropna().tolist()) #I use the title to get the topic\n",
    "vectorizer = CountVectorizer(analyzer='word', min_df=0.01, max_df=0.6, stop_words='english')\n",
    "\n",
    "X2 = vectorizer.fit_transform(normal.cleaned.dropna().tolist())\n",
    "\n",
    "df = pd.DataFrame(data={'word': vectorizer.get_feature_names(), \n",
    "                        'tf': X2.sum(axis=0).A1, \n",
    "                        'idf': tfidf_vectorizer.idf_,\n",
    "                        'tfidf': X.sum(axis=0).A1\n",
    "                       })\n",
    "print(\"Normal\")\n",
    "df = df.sort_values(['tfidf', 'tf', 'idf'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "\n",
    "smoothing = 0.001\n",
    "counts = defaultdict(lambda: defaultdict(lambda: smoothing))\n",
    "sarcasm[\"headline\"] = sarcasm[\"headline\"].astype(str)\n",
    "corpus = [line.strip().split() for line in sarcasm.headline.tolist()]\n",
    "\n",
    "#collect and count\n",
    "for sentence in corpus:\n",
    "    #include start and stop in the sentence\n",
    "    tokens = ['*', '*','*','*','*'] + sentence + ['STOP']\n",
    "    for u, v, w, x, y, z in nltk.ngrams(tokens, 6): #we create the ngrams from the sentence and we count them\n",
    "        counts[u, v, w, x, y][z] += 1\n",
    "\n",
    "def logP(u, v, w):\n",
    "    #(u,v,w) => P(w) preceeded by u,v we compute the log proba. to avoid numbers that are too small to divivde byand get infinty\n",
    "    return np.log(counts[(u, v)][w]) - np.log(sum(counts[(u, v)].values()))\n",
    "\n",
    "def sentence_logP(S):\n",
    "    #score a sentence in log likelihood with chain rule (product becomes sum with log) S: lits of strings\n",
    "    tokens = ['*', '*'] + S + ['STOP']\n",
    "    return sum([logP(u, v, w) for u, v, w in nltk.ngrams(tokens, 3)])\n",
    "    #for each sentence we eaxtract the trigrams and we compute their proba, and then sum it so we get the log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_word(u, v, w, x, y):\n",
    "    #sample a word v based on the history (u,v)\n",
    "    keys, values = zip(*counts[(u, v, w, x, y)].items()) #items returns a litsof tuples keys and values\n",
    "    #zip takes any number of arguments and zips them together \n",
    "    values = np.array(values)\n",
    "    values /= values.sum() # create probability distro\n",
    "    sample = np.random.multinomial(1, values) # pick one position (returns a position)\n",
    "    return keys[np.argmax(sample)]\n",
    "\n",
    "def generate2(initial=[]):\n",
    "    result = ['*', '*','*','*','*'] + initial\n",
    "    next_word = sample_next_word(result[-5], result[-4], result[-3],result[-2], result[-1])\n",
    "    result.append(next_word)\n",
    "    while next_word != 'STOP':\n",
    "        next_word = sample_next_word(result[-5], result[-4], result[-3],result[-2], result[-1])\n",
    "        result.append(next_word)\n",
    "\n",
    "    return ' '.join(result[2:-1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * john kerry scrambles to stop bunker's self-destruct sequence as russian oligarch taunts him from bank of monitors\n"
     ]
    }
   ],
   "source": [
    "print(generate2(['john']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * john boehner beheads juarez cartel member who dared muscle in on his legal weed turf\n"
     ]
    }
   ],
   "source": [
    "print(generate2(['john']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * john bolton: 'an attack on two saudi oil tankers is an attack on all americans'\n"
     ]
    }
   ],
   "source": [
    "print(generate2(['john']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * politicians ignoring the dangers of jowl implants\n"
     ]
    }
   ],
   "source": [
    "print(generate2(['politicians']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * metallica board of directors debates whether new riff will have negative impact on shareholder value\n"
     ]
    }
   ],
   "source": [
    "print(generate2([]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LDA topic model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dictionary\n",
      "Dictionary(42 unique tokens: ['get', 'work', 'come', 'use', 'area']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaMulticore, TfidfModel, CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "import time # to know how long training took\n",
    "import multiprocessing # to speed things up by parallelizing\n",
    "\n",
    "limit=7542\n",
    "\n",
    "\n",
    "\n",
    "# get dictionary\n",
    "sarcasm['verb'] = sarcasm.cleaned.apply(str)\n",
    "# run on 50000 instances\n",
    "instances = sarcasm.cleaned.dropna().apply(str.split)[:limit]\n",
    "print(\"creating dictionary\", flush=True)\n",
    "# read in instances and create Dictionary object w information about frequencies etc. \n",
    "dictionary = Dictionary(instances)\n",
    "# get rid of words that are too rare or too frequent\n",
    "dictionary.filter_extremes(no_below=100, no_above=0.3) #we restrict ousrlef with words that occur more than 50 times, and words that occur below 30% of the documents. \n",
    "print(dictionary, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translating corpus to IDs\n",
      "tf-idf transformation\n"
     ]
    }
   ],
   "source": [
    "#replace words by their numerical IDs and their frequency\n",
    "print(\"translating corpus to IDs\", flush=True)\n",
    "ldacorpus = [dictionary.doc2bow(text) for text in instances] #replace each words by its id in our vocabulary, list of list of integers\n",
    "# learn TFIDF values from corpus\n",
    "print(\"tf-idf transformation\", flush=True)\n",
    "tfidfmodel = TfidfModel(ldacorpus)\n",
    "# transform raw frequencies into TFIDF\n",
    "model_corpus = tfidfmodel[ldacorpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377.1\n",
      "fitting model\n",
      "done in 2.262817144393921\n"
     ]
    }
   ],
   "source": [
    "num_topics = 11\n",
    "\n",
    "# find chunksize to make about 200 updates\n",
    "num_passes = 10\n",
    "chunk_size = len(model_corpus) * num_passes/200\n",
    "print(chunk_size)\n",
    "\n",
    "start = time.time()\n",
    "print(\"fitting model\", flush=True)\n",
    "model = LdaMulticore(num_topics=num_topics, # number of topics\n",
    "                     corpus=model_corpus, # what to train on \n",
    "                     id2word=dictionary, # mapping from IDs to words\n",
    "                     workers=min(10, multiprocessing.cpu_count()-1), # choose 10 cores, or whatever computer has\n",
    "                     passes=num_passes, # make this many passes over data\n",
    "                     chunksize=chunk_size, # update after this many instances\n",
    "                     alpha=0.5 #controls how many topics we expcet to see in a document. if alpha is big we expect to see all topics in one document, large alpha is 100\n",
    "                    )\n",
    "    \n",
    "print(\"done in {}\".format(time.time()-start), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcasm Topics\n",
      "1 \"get\", \"nation\", \"come\", \"work\", \"just\"\n",
      "2 \"friend\", \"life\", \"new\", \"just\", \"year\"\n",
      "3 \"new\", \"now\", \"last\", \"man\", \"will\"\n",
      "4 \"child\", \"woman\", \"see\", \"will\", \"man\"\n",
      "5 \"old\", \"man\", \"-\", \"back\", \"would\"\n",
      "6 \"go\", \"time\", \"first\", \"say\", \"can\"\n",
      "7 \"make\", \"man\", \"day\", \"how\", \"just\"\n",
      "8 \"find\", \"study\", \"use\", \"people\", \"man\"\n",
      "9 \"report\", \"still\", \"more\", \"year\", \"can\"\n",
      "10 \"area\", \"only\", \"man\", \"look\", \"say\"\n",
      "11 \"take\", \"good\", \"family\", \"think\", \"woman\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# get the topic descritions\n",
    "topic_sep = re.compile(r\"0\\.[0-9]{3}\\*\") # getting rid of useless formatting\n",
    "# extract a list of tuples with topic number and descriptors from the model\n",
    "model_topics = [(topic_no, re.sub(topic_sep, '', model_topic).split(' + ')) for topic_no, model_topic in\n",
    "                model.print_topics(num_topics=num_topics, num_words=5)]\n",
    "\n",
    "print(\"Sarcasm Topics\")\n",
    "descriptors = []\n",
    "for i, m in model_topics:\n",
    "    print(i+1, \", \".join(m[:5]))\n",
    "    descriptors.append(\", \".join(m[:2]).replace('\"', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non sarcasm LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dictionary\n",
      "Dictionary(36 unique tokens: ['why', 'way', 'know', 'how', 'will']...)\n"
     ]
    }
   ],
   "source": [
    "# get dictionary\n",
    "normal['verb'] = normal.cleaned.apply(str)\n",
    "# run on 50000 instances\n",
    "instances = normal.cleaned.dropna().apply(str.split)[:limit]\n",
    "print(\"creating dictionary\", flush=True)\n",
    "# read in instances and create Dictionary object w information about frequencies etc. \n",
    "dictionary = Dictionary(instances)\n",
    "# get rid of words that are too rare or too frequent\n",
    "dictionary.filter_extremes(no_below=100, no_above=0.3) #we restrict ousrlef with words that occur more than 50 times, and words that occur below 30% of the documents. \n",
    "print(dictionary, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translating corpus to IDs\n",
      "tf-idf transformation\n"
     ]
    }
   ],
   "source": [
    "#replace words by their numerical IDs and their frequency\n",
    "print(\"translating corpus to IDs\", flush=True)\n",
    "ldacorpus = [dictionary.doc2bow(text) for text in instances] #replace each words by its id in our vocabulary, list of list of integers\n",
    "# learn TFIDF values from corpus\n",
    "print(\"tf-idf transformation\", flush=True)\n",
    "tfidfmodel = TfidfModel(ldacorpus)\n",
    "# transform raw frequencies into TFIDF\n",
    "model_corpus = tfidfmodel[ldacorpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377.1\n",
      "fitting model\n",
      "done in 2.280081033706665\n"
     ]
    }
   ],
   "source": [
    "num_topics = 11\n",
    "\n",
    "# find chunksize to make about 200 updates\n",
    "num_passes = 10\n",
    "chunk_size = len(model_corpus) * num_passes/200\n",
    "print(chunk_size)\n",
    "\n",
    "start = time.time()\n",
    "print(\"fitting model\", flush=True)\n",
    "model = LdaMulticore(num_topics=num_topics, # number of topics\n",
    "                     corpus=model_corpus, # what to train on \n",
    "                     id2word=dictionary, # mapping from IDs to words\n",
    "                     workers=min(10, multiprocessing.cpu_count()-1), # choose 10 cores, or whatever computer has\n",
    "                     passes=num_passes, # make this many passes over data\n",
    "                     chunksize=chunk_size, # update after this many instances\n",
    "                     alpha=0.5 #controls how many topics we expcet to see in a document. if alpha is big we expect to see all topics in one document, large alpha is 100\n",
    "                    )\n",
    "    \n",
    "print(\"done in {}\".format(time.time()-start), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOrmal headlines Topics\n",
      "1 \"make\", \"go\", \"woman\", \"man\", \"year\"\n",
      "2 \"world\", \"could\", \"may\", \"know\", \"get\"\n",
      "3 \"say\", \"man\", \"more\", \"life\", \"year\"\n",
      "4 \"will\", \"first\", \"want\", \"how\", \"new\"\n",
      "5 \"take\", \"can\", \"thing\", \"get\", \"say\"\n",
      "6 \"call\", \"just\", \"woman\", \"time\", \"get\"\n",
      "7 \"new\", \"-\", \"show\", \"life\", \"watch\"\n",
      "8 \"trump\", \"here\", \"should\", \"may\", \"make\"\n",
      "9 \"why\", \"people\", \"day\", \"will\", \"woman\"\n",
      "10 \"how\", \"can\", \"watch\", \"take\", \"woman\"\n",
      "11 \"good\", \"way\", \"know\", \"need\", \"life\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# get the topic descritions\n",
    "topic_sep = re.compile(r\"0\\.[0-9]{3}\\*\") # getting rid of useless formatting\n",
    "# extract a list of tuples with topic number and descriptors from the model\n",
    "model_topics = [(topic_no, re.sub(topic_sep, '', model_topic).split(' + ')) for topic_no, model_topic in\n",
    "                model.print_topics(num_topics=num_topics, num_words=5)]\n",
    "\n",
    "print(\"NOrmal headlines Topics\")\n",
    "descriptors = []\n",
    "for i, m in model_topics:\n",
    "    print(i+1, \", \".join(m[:5]))\n",
    "    descriptors.append(\", \".join(m[:2]).replace('\"', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction (15 pts)\n",
    "\n",
    "\n",
    "### 4.1 Classification (9 pts)\n",
    "Build a predictive model of the target label and use appropriate performance metrics. Your predictive analysis needs to involve **all** of the following, summarized in a table:\n",
    "\n",
    "1. a most-frequent-label baseline (1 point)\n",
    "2. a `LogisticRegression()` baseline with default parameters and 2-6 gram character TFIDF features (1 pt)\n",
    "3. the performance of **at least** two more predictive model architecture (2 pts each), including description/justification of the optmization steps taken (2 pts).\n",
    "4. two bootstrap sampling significance tests of the performance difference between your best model and each of the two baselines (1 pts)\n",
    "\n",
    "NB: Do make sure that the optimization steps are done on the development split and do *not* include the test split! Training on the test set will be graded 0!\n",
    "\n",
    "### 4.1 Structured Prediction (6pts)\n",
    "Run the Structured Prediction model as-is on your sequence prediction task, and note the performance as baseline (1 pt).\n",
    "Change the features to improve performance (2 pts).\n",
    "Run a suitable neural net implementation (in `keras`) on the data and compare the best performance to the other two models (4 pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizations (3 pts)\n",
    "\n",
    "Provide at least 3 visualizations of your work above. These can be in the respective sections. Use labels and legends. Be creative. Just please do not use word clouds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
