{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "The final project is designed to let you apply what you have learned so far, and demonstrate that you have mastered it. The submission will be graded on the correctness and performance of the execution of your analysis (50%), the ambitiousness of the problems chosen (30%), and the creativity of your questions and solutions (20%).\n",
    "\n",
    "Your submission should include all outputs and be *self-contained*, so it can be executed if necessary.\n",
    "\n",
    "The submission includes two parts:\n",
    "1. this notebook\n",
    "2. a 15-min presentation, to be held on May 8\n",
    "\n",
    "\n",
    "## Submission\n",
    "The project is due on ***May 07, 23:59 CET*** (counted as the time stamp when it is received). Late submissions will **not** be considered, and graded as 0! \n",
    "\n",
    "To submit, please:\n",
    "\n",
    "1. copy this file and all additional data into a folder with your group ID\n",
    "3. zip the folder\n",
    "4. send a copy of the zip file to Dirk Hovy <dirk.hovy@unibocconi.it> and Tommaso Fornaciari <fornaciari@unibocconi.it>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data, Preprocessing, and Annotation (4 pts)\n",
    "\n",
    "Find a data set for text classification and a data set for structured prediction. These can be the same.\n",
    "Kaggle is a good place to start, or the Google data set search. \n",
    "\n",
    "The data sets should have **at least 5,000** documents each. **At least 2000 instances** need to be labeled. If there is no label provided, you can annotate your own. You can get up to **3 bonus points** for annotation, depending on the amount and complexity of the annotation.\n",
    "\n",
    "Split the data into dedicated training, development, and test sets (if they do not include these already)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly (max. 100 words!) describe the content and type of the data set, and what you are planning to look at. \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data and explain (max. 200 words) which preprocessing steps you chose and why, and give statistics of the number of documents, types, and tokens, before and after preprocessing.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.theonion.com/thirtysomething-scien...   \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...   \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...   \n",
       "3  https://local.theonion.com/inclement-weather-p...   \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  thirtysomething scientists unveil doomsday clo...             1  \n",
       "1  dem rep. totally nails why congress is falling...             0  \n",
       "2  eat your veggies: 9 deliciously different recipes             0  \n",
       "3  inclement weather prevents liar from getting t...             1  \n",
       "4  mother comes pretty close to using word 'strea...             1  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "\n",
    "data = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocessing(sentence, entity_rec = False, POS_tagging_prep = True):\n",
    "    \"\"\"\n",
    "    Input: A document\n",
    "    Output: A cleaned, tokenised document\n",
    "    \"\"\"\n",
    "    s_list = []\n",
    "    # POS_tagging and preprocessing \n",
    "    if POS_tagging_prep == True:\n",
    "         return ' '.join([token.lemma_ for token in nlp(sentence) if (token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV', 'X'} and (token.lemma_.lower() != '’s') \n",
    "                                                                  and (token.is_stop == False) and (token.is_punct == False) \n",
    "                                                                  and (token.like_url == False))])\n",
    "                \n",
    "    # Tokenisation / stop-word removal / punctuation removal / url removal\n",
    "    else:\n",
    "        spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "        if entity_rec == True:\n",
    "            s_analysed = nlp(sentence)\n",
    "            # Entity recognition\n",
    "            entity = [entity.text for entity in s_analysed.ents]\n",
    "            for words in entity:\n",
    "                modified_words = words.replace(\" \", \"_\")\n",
    "                sentence = sentence.replace(words, modified_words)        \n",
    "        s_analysed = nlp(sentence.replace(\"’s\", \"\")) # removal of ’s before tokenisation \n",
    "        for token in s_analysed:\n",
    "            if (token.is_stop == False) and (token.is_punct == False) and (token.like_url == False):\n",
    "                s_list.append(token.lemma_.lower())\n",
    "    return s_list\n",
    "\n",
    "def verb(text):\n",
    "    '''\n",
    "    return the strings with the verbs only\n",
    "    '''\n",
    "    return ' '.join([token.lemma_ \n",
    "             for token in nlp(text) \n",
    "             if token.pos_ in {'VERB'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['cleaned'] = data.headline.apply(preprocessing, entity_rec = True)\n",
    "#data['verb'] = data.headline.apply(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 28619 article headlines\n",
      "Presence of NAN True\n",
      "0    14985\n",
      "1    13634\n",
      "Name: is_sarcastic, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff681453198>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data.drop(\"article_link\", axis = 1, inplace = True)\n",
    "\n",
    "data['Set'] = np.random.choice(\n",
    "    ['train', 'valid', 'test'],\n",
    "    data.shape[0],\n",
    "    p=[0.7, 0.15, 0.15]\n",
    ")\n",
    "\n",
    "print(\"There are\", data.shape[0],'article headlines')\n",
    "print(\"Presence of NAN\" , data.isna().values.any())\n",
    "print(data.is_sarcastic.value_counts())\n",
    "data.is_sarcastic.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cleaned_data.csv\")\n",
    "data.cleaned.dropna(axis = 0,inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_verb = data[[\"headline\",\"cleaned\",\"Set\",\"is_sarcastic\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>verb</th>\n",
       "      <th>Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientist hair loss</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>0</td>\n",
       "      <td>totally nail fall short gender racial equality</td>\n",
       "      <td>nail fall</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>0</td>\n",
       "      <td>eat veggie deliciously different recipe</td>\n",
       "      <td>eat</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevent liar get work</td>\n",
       "      <td>prevent get work</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>1</td>\n",
       "      <td>mother come pretty close word streaming correctly</td>\n",
       "      <td>come use</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           headline  \\\n",
       "0           0  thirtysomething scientists unveil doomsday clo...   \n",
       "1           1  dem rep. totally nails why congress is falling...   \n",
       "2           2  eat your veggies: 9 deliciously different recipes   \n",
       "3           3  inclement weather prevents liar from getting t...   \n",
       "4           4  mother comes pretty close to using word 'strea...   \n",
       "\n",
       "   is_sarcastic                                            cleaned  \\\n",
       "0             1                thirtysomething scientist hair loss   \n",
       "1             0     totally nail fall short gender racial equality   \n",
       "2             0            eat veggie deliciously different recipe   \n",
       "3             1            inclement weather prevent liar get work   \n",
       "4             1  mother come pretty close word streaming correctly   \n",
       "\n",
       "               verb    Set  \n",
       "0               NaN  train  \n",
       "1         nail fall  train  \n",
       "2               eat  train  \n",
       "3  prevent get work  train  \n",
       "4          come use   test  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_verb.cleaned.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugopao/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1494: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "sarcasm = no_verb.loc[no_verb[\"is_sarcastic\"] == 1,[\"headline\",\"cleaned\",\"verb\"]]\n",
    "normal = no_verb.loc[no_verb[\"is_sarcastic\"] == 0,[\"headline\",\"cleaned\",\"verb\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = no_verb.loc[no_verb.Set =='train', [\"cleaned\"]]\n",
    "y_train = no_verb.loc[no_verb.Set =='train', [\"is_sarcastic\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleaned    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.reset_index()\n",
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research Question(s) (2 pts)\n",
    "\n",
    "Describe what question you are investigating with the data (max. 100 words)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis (6 pts)\n",
    "\n",
    "Apply at least one version of *each* of the following analysis methods to the data set (justify your choices):\n",
    "1. Topic modeling with LDA (3 pts). Justify your choice of number of topics!\n",
    "2. Dense word embeddings ***or*** document embeddings: visualize these and show a clustering (3 pts) \n",
    "\n",
    "Your analysis needs to be run on the training data only! You can use the dev set for tuning.\n",
    "\n",
    "### Other methods\n",
    "If appropriate for your problem, feel free to explore other methods, as long as they do not require additional libraries (**up to 2 bonus points**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcasm Words\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nation</td>\n",
       "      <td>385</td>\n",
       "      <td>4.569496</td>\n",
       "      <td>308.890973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>woman</td>\n",
       "      <td>466</td>\n",
       "      <td>4.388481</td>\n",
       "      <td>360.380507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>report</td>\n",
       "      <td>573</td>\n",
       "      <td>4.166645</td>\n",
       "      <td>455.597051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>new</td>\n",
       "      <td>753</td>\n",
       "      <td>3.905490</td>\n",
       "      <td>615.215899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>man</td>\n",
       "      <td>1001</td>\n",
       "      <td>3.612780</td>\n",
       "      <td>743.379542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word    tf       idf       tfidf\n",
       "18  nation   385  4.569496  308.890973\n",
       "37   woman   466  4.388481  360.380507\n",
       "24  report   573  4.166645  455.597051\n",
       "19     new   753  3.905490  615.215899\n",
       "17     man  1001  3.612780  743.379542"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                             min_df=0.01, #df stands for document frequency\n",
    "                             max_df=0.6, \n",
    "                             stop_words='english')\n",
    "X = tfidf_vectorizer.fit_transform(sarcasm.cleaned.dropna().tolist())\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', min_df=0.01, max_df=0.6, stop_words='english')\n",
    "\n",
    "X2 = vectorizer.fit_transform(sarcasm.cleaned.dropna().tolist())\n",
    "\n",
    "df = pd.DataFrame(data={'word': vectorizer.get_feature_names(), \n",
    "                        'tf': X2.sum(axis=0).A1, \n",
    "                        'idf': tfidf_vectorizer.idf_,\n",
    "                        'tfidf': X.sum(axis=0).A1\n",
    "                       })\n",
    "print(\"Sarcasm Words\")\n",
    "df = df.sort_values(['tfidf', 'tf', 'idf'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>man</td>\n",
       "      <td>314</td>\n",
       "      <td>4.860292</td>\n",
       "      <td>256.871140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>say</td>\n",
       "      <td>395</td>\n",
       "      <td>4.630145</td>\n",
       "      <td>344.223808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>woman</td>\n",
       "      <td>473</td>\n",
       "      <td>4.473098</td>\n",
       "      <td>389.389017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>new</td>\n",
       "      <td>477</td>\n",
       "      <td>4.447400</td>\n",
       "      <td>400.367527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>trump</td>\n",
       "      <td>498</td>\n",
       "      <td>4.397906</td>\n",
       "      <td>432.726904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word   tf       idf       tfidf\n",
       "14    man  314  4.860292  256.871140\n",
       "21    say  395  4.630145  344.223808\n",
       "31  woman  473  4.473098  389.389017\n",
       "16    new  477  4.447400  400.367527\n",
       "25  trump  498  4.397906  432.726904"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                             min_df=0.01, \n",
    "                             max_df=0.6, \n",
    "                             stop_words='english')\n",
    "X = tfidf_vectorizer.fit_transform(normal.cleaned.dropna().tolist()) #I use the title to get the topic\n",
    "vectorizer = CountVectorizer(analyzer='word', min_df=0.01, max_df=0.6, stop_words='english')\n",
    "\n",
    "X2 = vectorizer.fit_transform(normal.cleaned.dropna().tolist())\n",
    "\n",
    "df = pd.DataFrame(data={'word': vectorizer.get_feature_names(), \n",
    "                        'tf': X2.sum(axis=0).A1, \n",
    "                        'idf': tfidf_vectorizer.idf_,\n",
    "                        'tfidf': X.sum(axis=0).A1\n",
    "                       })\n",
    "print(\"Normal\")\n",
    "df = df.sort_values(['tfidf', 'tf', 'idf'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "\n",
    "smoothing = 0.001\n",
    "counts = defaultdict(lambda: defaultdict(lambda: smoothing))\n",
    "sarcasm[\"headline\"] = sarcasm[\"headline\"].astype(str)\n",
    "corpus = [line.strip().split() for line in sarcasm.headline.tolist()]\n",
    "\n",
    "#collect and count\n",
    "for sentence in corpus:\n",
    "    #include start and stop in the sentence\n",
    "    tokens = ['*', '*','*','*','*'] + sentence + ['STOP']\n",
    "    for u, v, w, x, y, z in nltk.ngrams(tokens, 6): #we create the ngrams from the sentence and we count them\n",
    "        counts[u, v, w, x, y][z] += 1\n",
    "\n",
    "def logP(u, v, w):\n",
    "    #(u,v,w) => P(w) preceeded by u,v we compute the log proba. to avoid numbers that are too small to divivde byand get infinty\n",
    "    return np.log(counts[(u, v)][w]) - np.log(sum(counts[(u, v)].values()))\n",
    "\n",
    "def sentence_logP(S):\n",
    "    #score a sentence in log likelihood with chain rule (product becomes sum with log) S: lits of strings\n",
    "    tokens = ['*', '*'] + S + ['STOP']\n",
    "    return sum([logP(u, v, w) for u, v, w in nltk.ngrams(tokens, 3)])\n",
    "    #for each sentence we eaxtract the trigrams and we compute their proba, and then sum it so we get the log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_word(u, v, w, x, y):\n",
    "    #sample a word v based on the history (u,v)\n",
    "    keys, values = zip(*counts[(u, v, w, x, y)].items()) #items returns a litsof tuples keys and values\n",
    "    #zip takes any number of arguments and zips them together \n",
    "    values = np.array(values)\n",
    "    values /= values.sum() # create probability distro\n",
    "    sample = np.random.multinomial(1, values) # pick one position (returns a position)\n",
    "    return keys[np.argmax(sample)]\n",
    "\n",
    "def generate2(initial=[]):\n",
    "    result = ['*', '*','*','*','*'] + initial\n",
    "    next_word = sample_next_word(result[-5], result[-4], result[-3],result[-2], result[-1])\n",
    "    result.append(next_word)\n",
    "    while next_word != 'STOP':\n",
    "        next_word = sample_next_word(result[-5], result[-4], result[-3],result[-2], result[-1])\n",
    "        result.append(next_word)\n",
    "\n",
    "    return ' '.join(result[2:-1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * john bolton insists iran likely harboring dangerous terrorist osama bin laden\n"
     ]
    }
   ],
   "source": [
    "print(generate2(['john']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * john edwards pays $30 to register edwards2016.com just in case\n"
     ]
    }
   ],
   "source": [
    "print(generate2(['john']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * john kelly resigns in last-ditch effort to save his and trump's friendship\n"
     ]
    }
   ],
   "source": [
    "print(generate2(['john']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * politicians ignoring the dangers of jowl implants\n"
     ]
    }
   ],
   "source": [
    "print(generate2(['politicians']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * universe feels zero connection to guy tripping on mushrooms\n"
     ]
    }
   ],
   "source": [
    "print(generate2([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#checking if the generated headlines is in the training data\n",
    "print(sarcasm[\"headline\"].str.contains(\"universe feels zero connection to guy tripping on mushrooms\").any())\n",
    "print(sarcasm[\"headline\"].str.contains(\"politicians ignoring the dangers of jowl implants\").any())\n",
    "print(sarcasm[\"headline\"].str.contains(\"john bolton insists iran likely harboring dangerous terrorist osama bin laden\").any())\n",
    "print(sarcasm[\"headline\"].str.contains(\"john edwards pays $30 to register edwards2016.com just in case\").any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying with 2grams because with 6grams its too specific and the language model just spits out headlines from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "for sentence in corpus:\n",
    "    tokens = ['*', '*'] + sentence + ['STOP']\n",
    "    for u, v, w in nltk.ngrams(tokens, 3):\n",
    "        counts[(u, v)][w] += 1\n",
    "\n",
    "def logP(u, v, w):\n",
    "    return np.log(counts[(u, v)][w]) - np.log(sum(counts[(u, v)].values()))\n",
    "\n",
    "def sentence_logP(S):\n",
    "    tokens = ['*', '*'] + S + ['STOP']\n",
    "    return sum([logP(u, v, w) for u, v, w in nltk.ngrams(tokens, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_word(u, v):\n",
    "    keys, values = zip(*counts[(u, v)].items())\n",
    "    values = np.array(values)\n",
    "    values /= values.sum() # create probability distro\n",
    "    sample = np.random.multinomial(1, values) # pick one position\n",
    "    return keys[np.argmax(sample)]\n",
    "\n",
    "def generate():\n",
    "    result = ['*', '*']\n",
    "    next_word = sample_next_word(result[-2], result[-1])\n",
    "    result.append(next_word)\n",
    "    while next_word != 'STOP':\n",
    "        next_word = sample_next_word(result[-2], result[-1])\n",
    "        result.append(next_word)\n",
    "\n",
    "    return ' '.join(result[2:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dog can't believe he left consulate\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deceased souls backed up at tall building thinking about, you know'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sighing trump sexual assault has cooled down'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'increasingly desperate advertisers settle for more attainable 35-to-44-year-old demographic'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'matt lauer returns to stage for humans'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(sarcasm[\"headline\"].str.contains(\"dog can't believe he left consulate\").any())\n",
    "print(sarcasm[\"headline\"].str.contains(\"deceased souls backed up at tall building thinking about, you know\").any())\n",
    "print(sarcasm[\"headline\"].str.contains(\"sighing trump sexual assault has cooled down\").any())\n",
    "print(sarcasm[\"headline\"].str.contains(\"matt lauer returns to stage for humans\").any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LDA topic model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dictionary\n",
      "Dictionary(74 unique tokens: ['get', 'work', 'come', 'run', 'area']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaMulticore, TfidfModel, CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "import time # to know how long training took\n",
    "import multiprocessing # to speed things up by parallelizing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get dictionary\n",
    "instances = sarcasm.cleaned.dropna().apply(str.split)\n",
    "print(\"creating dictionary\", flush=True)\n",
    "# read in instances and create Dictionary object w information about frequencies etc. \n",
    "dictionary = Dictionary(instances)\n",
    "# get rid of words that are too rare or too frequent\n",
    "dictionary.filter_extremes(no_below=100, no_above=0.3) #we restrict ousrlef with words that occur more than 50 times, and words that occur below 30% of the documents. \n",
    "print(dictionary, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             [thirtysomething, scientist, hair, loss]\n",
       "3       [inclement, weather, prevent, liar, get, work]\n",
       "4    [mother, come, pretty, close, word, streaming,...\n",
       "7    [warm, donation, nearly, cost, fail, balloon, ...\n",
       "8         [shadow, government, get, large, meet, room]\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translating corpus to IDs\n",
      "tf-idf transformation\n"
     ]
    }
   ],
   "source": [
    "#replace words by their numerical IDs and their frequency\n",
    "print(\"translating corpus to IDs\", flush=True)\n",
    "ldacorpus = [dictionary.doc2bow(text) for text in instances] #replace each words by its id in our vocabulary, list of list of integers\n",
    "# learn TFIDF values from corpus\n",
    "print(\"tf-idf transformation\", flush=True)\n",
    "tfidfmodel = TfidfModel(ldacorpus)\n",
    "# transform raw frequencies into TFIDF\n",
    "model_corpus = tfidfmodel[ldacorpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679.75\n",
      "fitting model\n",
      "done in 3.3294999599456787\n"
     ]
    }
   ],
   "source": [
    "num_topics = 11\n",
    "\n",
    "# find chunksize to make about 200 updates\n",
    "num_passes = 10\n",
    "chunk_size = len(model_corpus) * num_passes/200\n",
    "print(chunk_size)\n",
    "\n",
    "start = time.time()\n",
    "print(\"fitting model\", flush=True)\n",
    "model = LdaMulticore(num_topics=num_topics, # number of topics\n",
    "                     corpus=model_corpus, # what to train on \n",
    "                     id2word=dictionary, # mapping from IDs to words\n",
    "                     workers=min(10, multiprocessing.cpu_count()-1), # choose 10 cores, or whatever computer has\n",
    "                     passes=num_passes, # make this many passes over data\n",
    "                     chunksize=chunk_size, # update after this many instances\n",
    "                     alpha=0.5 #controls how many topics we expcet to see in a document. if alpha is big we expect to see all topics in one document, large alpha is 100\n",
    "                    )\n",
    "    \n",
    "print(\"done in {}\".format(time.time()-start), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcasm Topics\n",
      "1 \"friend\", \"call\", \"kill\", \"reveal\", \"ask\"\n",
      "2 \"nation\", \"man\", \"come\", \"find\", \"take\"\n",
      "3 \"man\", \"think\", \"area\", \"run\", \"new\"\n",
      "4 \"look\", \"man\", \"people\", \"little\", \"life\"\n",
      "5 \"get\", \"give\", \"spend\", \"trump\", \"try\"\n",
      "6 \"new\", \"year\", \"thing\", \"guy\", \"local\"\n",
      "7 \"go\", \"family\", \"death\", \"school\", \"want\"\n",
      "8 \"know\", \"way\", \"watch\", \"time\", \"mom\"\n",
      "9 \"introduce\", \"plan\", \"man\", \"report\", \"feel\"\n",
      "10 \"old\", \"woman\", \"study\", \"world\", \"couple\"\n",
      "11 \"day\", \"self\", \"child\", \"report\", \"announce\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# get the topic descritions\n",
    "topic_sep = re.compile(r\"0\\.[0-9]{3}\\*\") # getting rid of useless formatting\n",
    "# extract a list of tuples with topic number and descriptors from the model\n",
    "model_topics = [(topic_no, re.sub(topic_sep, '', model_topic).split(' + ')) for topic_no, model_topic in\n",
    "                model.print_topics(num_topics=num_topics, num_words=5)]\n",
    "\n",
    "print(\"Sarcasm Topics\")\n",
    "descriptors = []\n",
    "for i, m in model_topics:\n",
    "    print(i+1, \", \".join(m[:5]))\n",
    "    descriptors.append(\", \".join(m[:2]).replace('\"', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non sarcasm LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dictionary\n",
      "Dictionary(85 unique tokens: ['way', 'know', 'parent', 'tell', 'leave']...)\n"
     ]
    }
   ],
   "source": [
    "# get dictionary\n",
    "normal['verb'] = normal.cleaned.apply(str)\n",
    "# run on 50000 instances\n",
    "instances = normal.cleaned.dropna().apply(str.split)\n",
    "print(\"creating dictionary\", flush=True)\n",
    "# read in instances and create Dictionary object w information about frequencies etc. \n",
    "dictionary = Dictionary(instances)\n",
    "# get rid of words that are too rare or too frequent\n",
    "dictionary.filter_extremes(no_below=100, no_above=0.3) #we restrict ousrlef with words that occur more than 50 times, and words that occur below 30% of the documents. \n",
    "print(dictionary, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translating corpus to IDs\n",
      "tf-idf transformation\n"
     ]
    }
   ],
   "source": [
    "#replace words by their numerical IDs and their frequency\n",
    "print(\"translating corpus to IDs\", flush=True)\n",
    "ldacorpus = [dictionary.doc2bow(text) for text in instances] #replace each words by its id in our vocabulary, list of list of integers\n",
    "# learn TFIDF values from corpus\n",
    "print(\"tf-idf transformation\", flush=True)\n",
    "tfidfmodel = TfidfModel(ldacorpus)\n",
    "# transform raw frequencies into TFIDF\n",
    "model_corpus = tfidfmodel[ldacorpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743.0\n",
      "fitting model\n",
      "done in 4.317033290863037\n"
     ]
    }
   ],
   "source": [
    "num_topics = 11\n",
    "\n",
    "# find chunksize to make about 200 updates\n",
    "num_passes = 10\n",
    "chunk_size = len(model_corpus) * num_passes/200\n",
    "print(chunk_size)\n",
    "\n",
    "start = time.time()\n",
    "print(\"fitting model\", flush=True)\n",
    "model = LdaMulticore(num_topics=num_topics, # number of topics\n",
    "                     corpus=model_corpus, # what to train on \n",
    "                     id2word=dictionary, # mapping from IDs to words\n",
    "                     workers=min(10, multiprocessing.cpu_count()-1), # choose 10 cores, or whatever computer has\n",
    "                     passes=num_passes, # make this many passes over data\n",
    "                     chunksize=chunk_size, # update after this many instances\n",
    "                     alpha=0.5 #controls how many topics we expcet to see in a document. if alpha is big we expect to see all topics in one document, large alpha is 100\n",
    "                    )\n",
    "    \n",
    "print(\"done in {}\".format(time.time()-start), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOrmal headlines Topics\n",
      "1 \"right\", \"make\", \"war\", \"take\", \"think\"\n",
      "2 \"year\", \"thing\", \"day\", \"bad\", \"real\"\n",
      "3 \"good\", \"watch\", \"school\", \"fire\", \"let\"\n",
      "4 \"trump\", \"say\", \"plan\", \"know\", \"fight\"\n",
      "5 \"people\", \"talk\", \"tell\", \"want\", \"climate\"\n",
      "6 \"new\", \"attack\", \"go\", \"care\", \"health\"\n",
      "7 \"get\", \"photo\", \"campaign\", \"leave\", \"time\"\n",
      "8 \"video\", \"death\", \"live\", \"student\", \"family\"\n",
      "9 \"old\", \"gun\", \"police\", \"work\", \"star\"\n",
      "10 \"woman\", \"man\", \"help\", \"american\", \"week\"\n",
      "11 \"win\", \"come\", \"election\", \"world\", \"find\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# get the topic descritions\n",
    "topic_sep = re.compile(r\"0\\.[0-9]{3}\\*\") # getting rid of useless formatting\n",
    "# extract a list of tuples with topic number and descriptors from the model\n",
    "model_topics = [(topic_no, re.sub(topic_sep, '', model_topic).split(' + ')) for topic_no, model_topic in\n",
    "                model.print_topics(num_topics=num_topics, num_words=5)]\n",
    "\n",
    "print(\"NOrmal headlines Topics\")\n",
    "descriptors = []\n",
    "for i, m in model_topics:\n",
    "    print(i+1, \", \".join(m[:5]))\n",
    "    descriptors.append(\", \".join(m[:2]).replace('\"', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Result : sarcasm doesn't differentiate itself by the topic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction (15 pts)\n",
    "\n",
    "\n",
    "### 4.1 Classification (9 pts)\n",
    "Build a predictive model of the target label and use appropriate performance metrics. Your predictive analysis needs to involve **all** of the following, summarized in a table:\n",
    "\n",
    "1. a most-frequent-label baseline (1 point)\n",
    "2. a `LogisticRegression()` baseline with default parameters and 2-6 gram character TFIDF features (1 pt)\n",
    "3. the performance of **at least** two more predictive model architecture (2 pts each), including description/justification of the optmization steps taken (2 pts).\n",
    "4. two bootstrap sampling significance tests of the performance difference between your best model and each of the two baselines (1 pts)\n",
    "\n",
    "NB: Do make sure that the optimization steps are done on the development split and do *not* include the test split! Training on the test set will be graded 0!\n",
    "\n",
    "### 4.1 Structured Prediction (6pts)\n",
    "Run the Structured Prediction model as-is on your sequence prediction task, and note the performance as baseline (1 pt).\n",
    "Change the features to improve performance (2 pts).\n",
    "Run a suitable neural net implementation (in `keras`) on the data and compare the best performance to the other two models (4 pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      1.00      0.68      2241\n",
      "           1       0.00      0.00      0.00      2075\n",
      "\n",
      "    accuracy                           0.52      4316\n",
      "   macro avg       0.26      0.50      0.34      4316\n",
      "weighted avg       0.27      0.52      0.35      4316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugopao/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "\n",
    "X_dev = no_verb.loc[no_verb.Set =='valid', [\"cleaned\"]]\n",
    "y_dev =no_verb.loc[no_verb.Set =='valid', [\"is_sarcastic\"]]\n",
    "\n",
    "most_frequent = DummyClassifier(strategy='most_frequent')\n",
    "most_frequent.fit(X_train, y_train)\n",
    "dumb_predictions = most_frequent.predict(X_dev)\n",
    "\n",
    "print(classification_report(y_dev, dumb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thirtysomething scientist hair loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>totally nail fall short gender racial equality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eat veggie deliciously different recipe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inclement weather prevent liar get work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>white inheritance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          cleaned\n",
       "0             thirtysomething scientist hair loss\n",
       "1  totally nail fall short gender racial equality\n",
       "2         eat veggie deliciously different recipe\n",
       "3         inclement weather prevent liar get work\n",
       "5                               white inheritance"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19803, 1)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleaned    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19803, 38)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), \n",
    "                             min_df=0.01, \n",
    "                             max_df=0.7, \n",
    "                             analyzer='word')\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train.cleaned.tolist())\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugopao/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.6 ms, sys: 105 ms, total: 159 ms\n",
      "Wall time: 408 ms\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=-1, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs')\n",
    "%time classifier.fit(X_train, y_train)\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.85      0.69      2241\n",
      "           1       0.67      0.32      0.43      2075\n",
      "\n",
      "    accuracy                           0.60      4316\n",
      "   macro avg       0.62      0.59      0.56      4316\n",
      "weighted avg       0.62      0.60      0.57      4316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_dev = vectorizer.transform(X_dev.cleaned.tolist())\n",
    "predictions = classifier.predict(X_dev)\n",
    "print(classification_report(y_dev,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugopao/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.77 ms, sys: 4.1 ms, total: 13.9 ms\n",
      "Wall time: 213 ms\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.83      0.68      2241\n",
      "           1       0.65      0.35      0.45      2075\n",
      "\n",
      "    accuracy                           0.60      4316\n",
      "   macro avg       0.62      0.59      0.57      4316\n",
      "weighted avg       0.62      0.60      0.57      4316\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.83      0.68      2241\n",
      "           1       0.65      0.35      0.45      2075\n",
      "\n",
      "    accuracy                           0.60      4316\n",
      "   macro avg       0.62      0.59      0.57      4316\n",
      "weighted avg       0.62      0.60      0.57      4316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_balanced = LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', \n",
    "                                         class_weight='balanced')\n",
    "%time classifier_balanced.fit(X_train, y_train)\n",
    "predictions_balanced = classifier_balanced.predict(X_dev)\n",
    "\n",
    "\n",
    "print(classification_report(y_dev, predictions_balanced))\n",
    "\n",
    "predictions_balanced_test = classifier_balanced.predict(X_dev)\n",
    "print(classification_report(y_dev, predictions_balanced_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugopao/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/hugopao/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.86      0.69      2241\n",
      "           1       0.67      0.30      0.42      2075\n",
      "\n",
      "    accuracy                           0.59      4316\n",
      "   macro avg       0.62      0.58      0.55      4316\n",
      "weighted avg       0.62      0.59      0.56      4316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "clf = xgboost.XGBClassifier(n_estimators = 1000)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions_boost = clf.predict(X_dev)\n",
    "print(classification_report(y_dev, predictions_boost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19803/19803 [==============================] - 1s 28us/step - loss: 0.6671 - accuracy: 0.5908\n",
      "Epoch 2/50\n",
      "19803/19803 [==============================] - 0s 22us/step - loss: 0.6603 - accuracy: 0.5954\n",
      "Epoch 3/50\n",
      "19803/19803 [==============================] - 0s 21us/step - loss: 0.6581 - accuracy: 0.5992\n",
      "Epoch 4/50\n",
      "19803/19803 [==============================] - 0s 21us/step - loss: 0.6578 - accuracy: 0.6012\n",
      "Epoch 5/50\n",
      "19803/19803 [==============================] - 0s 21us/step - loss: 0.6571 - accuracy: 0.6008\n",
      "Epoch 6/50\n",
      "19803/19803 [==============================] - 0s 21us/step - loss: 0.6564 - accuracy: 0.6022\n",
      "Epoch 7/50\n",
      "19803/19803 [==============================] - 0s 21us/step - loss: 0.6558 - accuracy: 0.6009\n",
      "Epoch 8/50\n",
      "19803/19803 [==============================] - 0s 22us/step - loss: 0.6543 - accuracy: 0.6034\n",
      "Epoch 9/50\n",
      "19803/19803 [==============================] - 0s 21us/step - loss: 0.6544 - accuracy: 0.6043\n",
      "Epoch 10/50\n",
      "19803/19803 [==============================] - 0s 21us/step - loss: 0.6531 - accuracy: 0.6032\n",
      "Epoch 11/50\n",
      "19803/19803 [==============================] - 0s 21us/step - loss: 0.6528 - accuracy: 0.6042\n",
      "Epoch 12/50\n",
      "19803/19803 [==============================] - 0s 21us/step - loss: 0.6521 - accuracy: 0.6047\n",
      "Epoch 13/50\n",
      "19803/19803 [==============================] - 0s 22us/step - loss: 0.6519 - accuracy: 0.6069\n",
      "Epoch 14/50\n",
      "19803/19803 [==============================] - 0s 24us/step - loss: 0.6505 - accuracy: 0.6052\n",
      "Epoch 15/50\n",
      "19803/19803 [==============================] - 0s 22us/step - loss: 0.6508 - accuracy: 0.6071\n",
      "Epoch 16/50\n",
      "19803/19803 [==============================] - 0s 23us/step - loss: 0.6505 - accuracy: 0.6066\n",
      "Epoch 17/50\n",
      "19803/19803 [==============================] - 0s 22us/step - loss: 0.6483 - accuracy: 0.6050\n",
      "Epoch 18/50\n",
      "19803/19803 [==============================] - 0s 22us/step - loss: 0.6487 - accuracy: 0.6061\n",
      "Epoch 19/50\n",
      "19803/19803 [==============================] - 0s 22us/step - loss: 0.6464 - accuracy: 0.6087\n",
      "Epoch 20/50\n",
      "19803/19803 [==============================] - 0s 22us/step - loss: 0.6477 - accuracy: 0.6075\n",
      "Epoch 21/50\n",
      "19803/19803 [==============================] - 0s 23us/step - loss: 0.6466 - accuracy: 0.6096\n",
      "Epoch 22/50\n",
      "19803/19803 [==============================] - 0s 21us/step - loss: 0.6459 - accuracy: 0.6070\n",
      "Epoch 23/50\n",
      "19803/19803 [==============================] - 0s 22us/step - loss: 0.6448 - accuracy: 0.6105\n",
      "Epoch 24/50\n",
      "19803/19803 [==============================] - 0s 24us/step - loss: 0.6445 - accuracy: 0.6095\n",
      "Epoch 25/50\n",
      "19803/19803 [==============================] - 0s 25us/step - loss: 0.6446 - accuracy: 0.6095\n",
      "Epoch 26/50\n",
      "19803/19803 [==============================] - 0s 23us/step - loss: 0.6442 - accuracy: 0.6100\n",
      "Epoch 27/50\n",
      "19803/19803 [==============================] - 0s 24us/step - loss: 0.6423 - accuracy: 0.6117\n",
      "Epoch 28/50\n",
      "19803/19803 [==============================] - 0s 24us/step - loss: 0.6428 - accuracy: 0.6121\n",
      "Epoch 29/50\n",
      "19803/19803 [==============================] - 0s 22us/step - loss: 0.6411 - accuracy: 0.6121\n",
      "Epoch 30/50\n",
      "19803/19803 [==============================] - 0s 22us/step - loss: 0.6414 - accuracy: 0.6117\n",
      "Epoch 31/50\n",
      "19803/19803 [==============================] - 0s 23us/step - loss: 0.6407 - accuracy: 0.6123\n",
      "Epoch 32/50\n",
      "19803/19803 [==============================] - 0s 22us/step - loss: 0.6415 - accuracy: 0.6107\n",
      "Epoch 33/50\n",
      "19803/19803 [==============================] - 0s 22us/step - loss: 0.6399 - accuracy: 0.6130\n",
      "Epoch 34/50\n",
      "19803/19803 [==============================] - 0s 21us/step - loss: 0.6388 - accuracy: 0.6127\n",
      "Epoch 35/50\n",
      "19803/19803 [==============================] - 0s 25us/step - loss: 0.6382 - accuracy: 0.6140\n",
      "Epoch 36/50\n",
      "19803/19803 [==============================] - 0s 25us/step - loss: 0.6390 - accuracy: 0.6125\n",
      "Epoch 37/50\n",
      "19803/19803 [==============================] - 0s 24us/step - loss: 0.6394 - accuracy: 0.6122\n",
      "Epoch 38/50\n",
      "19803/19803 [==============================] - 0s 22us/step - loss: 0.6385 - accuracy: 0.6133\n",
      "Epoch 39/50\n",
      "19803/19803 [==============================] - 0s 23us/step - loss: 0.6368 - accuracy: 0.6144\n",
      "Epoch 40/50\n",
      "19803/19803 [==============================] - 0s 22us/step - loss: 0.6365 - accuracy: 0.6149\n",
      "Epoch 41/50\n",
      "19803/19803 [==============================] - 0s 22us/step - loss: 0.6362 - accuracy: 0.6141\n",
      "Epoch 42/50\n",
      "19803/19803 [==============================] - 0s 23us/step - loss: 0.6377 - accuracy: 0.6142\n",
      "Epoch 43/50\n",
      "19803/19803 [==============================] - 0s 23us/step - loss: 0.6358 - accuracy: 0.6144\n",
      "Epoch 44/50\n",
      "19803/19803 [==============================] - 0s 23us/step - loss: 0.6368 - accuracy: 0.6152\n",
      "Epoch 45/50\n",
      "19803/19803 [==============================] - 0s 24us/step - loss: 0.6346 - accuracy: 0.6156\n",
      "Epoch 46/50\n",
      "19803/19803 [==============================] - 0s 24us/step - loss: 0.6347 - accuracy: 0.6138\n",
      "Epoch 47/50\n",
      "19803/19803 [==============================] - 0s 25us/step - loss: 0.6341 - accuracy: 0.6142\n",
      "Epoch 48/50\n",
      "19803/19803 [==============================] - 0s 24us/step - loss: 0.6337 - accuracy: 0.6159\n",
      "Epoch 49/50\n",
      "19803/19803 [==============================] - 0s 22us/step - loss: 0.6335 - accuracy: 0.6157\n",
      "Epoch 50/50\n",
      "19803/19803 [==============================] - 0s 23us/step - loss: 0.6343 - accuracy: 0.6152\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim = X_train.shape[1]))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 50\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.86      0.69      2241\n",
      "           1       0.67      0.30      0.42      2075\n",
      "\n",
      "    accuracy                           0.59      4316\n",
      "   macro avg       0.62      0.58      0.55      4316\n",
      "weighted avg       0.62      0.59      0.56      4316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_deep = model.predict(X_dev)\n",
    "print(classification_report(y_dev, predictions_boost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_LENGTH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-4e6458e133f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m inputs = Input((MAX_LENGTH, ), \n\u001b[0m\u001b[1;32m     11\u001b[0m                name='word_IDs')\n\u001b[1;32m     12\u001b[0m embeddings = Embedding(input_dim=len(word2int), \n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_LENGTH' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding\n",
    "from keras.layers import Bidirectional, LSTM\n",
    "from keras.layers import Dropout, Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "inputs = Input((MAX_LENGTH, ), \n",
    "               name='word_IDs')\n",
    "embeddings = Embedding(input_dim=len(word2int), \n",
    "                       output_dim=128, \n",
    "                       mask_zero=True, \n",
    "                       name='embeddings')(inputs)\n",
    "lstm = LSTM(units=256,\n",
    "              return_sequences=True,\n",
    "              name=\"LSTM\")(embeddings)\n",
    "dropout = Dropout(0.3, name='dropout')(lstm)\n",
    "lstm_out = Dense(len(tag2int), name='output')(dropout)\n",
    "output = Activation('softmax', name='softmax')(lstm_out)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizations (3 pts)\n",
    "\n",
    "Provide at least 3 visualizations of your work above. These can be in the respective sections. Use labels and legends. Be creative. Just please do not use word clouds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
