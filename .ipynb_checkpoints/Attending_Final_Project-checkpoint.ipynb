{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "The final project is designed to let you apply what you have learned so far, and demonstrate that you have mastered it. The submission will be graded on the correctness and performance of the execution of your analysis (50%), the ambitiousness of the problems chosen (30%), and the creativity of your questions and solutions (20%).\n",
    "\n",
    "Your submission should include all outputs and be *self-contained*, so it can be executed if necessary.\n",
    "\n",
    "The submission includes two parts:\n",
    "1. this notebook\n",
    "2. a 15-min presentation, to be held on May 8\n",
    "\n",
    "\n",
    "## Submission\n",
    "The project is due on ***May 07, 23:59 CET*** (counted as the time stamp when it is received). Late submissions will **not** be considered, and graded as 0! \n",
    "\n",
    "To submit, please:\n",
    "\n",
    "1. copy this file and all additional data into a folder with your group ID\n",
    "3. zip the folder\n",
    "4. send a copy of the zip file to Dirk Hovy <dirk.hovy@unibocconi.it> and Tommaso Fornaciari <fornaciari@unibocconi.it>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data, Preprocessing, and Annotation (4 pts)\n",
    "\n",
    "Find a data set for text classification and a data set for structured prediction. These can be the same.\n",
    "Kaggle is a good place to start, or the Google data set search. \n",
    "\n",
    "The data sets should have **at least 5,000** documents each. **At least 2000 instances** need to be labeled. If there is no label provided, you can annotate your own. You can get up to **3 bonus points** for annotation, depending on the amount and complexity of the annotation.\n",
    "\n",
    "Split the data into dedicated training, development, and test sets (if they do not include these already)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly (max. 100 words!) describe the content and type of the data set, and what you are planning to look at. \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data and explain (max. 200 words) which preprocessing steps you chose and why, and give statistics of the number of documents, types, and tokens, before and after preprocessing.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.theonion.com/thirtysomething-scien...   \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...   \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...   \n",
       "3  https://local.theonion.com/inclement-weather-p...   \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  thirtysomething scientists unveil doomsday clo...             1  \n",
       "1  dem rep. totally nails why congress is falling...             0  \n",
       "2  eat your veggies: 9 deliciously different recipes             0  \n",
       "3  inclement weather prevents liar from getting t...             1  \n",
       "4  mother comes pretty close to using word 'strea...             1  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "\n",
    "data = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocessing(sentence, entity_rec = False, POS_tagging_prep = True):\n",
    "    \"\"\"\n",
    "    Input: A document\n",
    "    Output: A cleaned, tokenised document\n",
    "    \"\"\"\n",
    "    s_list = []\n",
    "    # POS_tagging and preprocessing \n",
    "    if POS_tagging_prep == True:\n",
    "         return ' '.join([token.lemma_ for token in nlp(sentence) if (token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV', 'X'} and (token.lemma_.lower() != '’s') \n",
    "                                                                  and (token.is_stop == False) and (token.is_punct == False) \n",
    "                                                                  and (token.like_url == False))])\n",
    "                \n",
    "    # Tokenisation / stop-word removal / punctuation removal / url removal\n",
    "    else:\n",
    "        spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "        if entity_rec == True:\n",
    "            s_analysed = nlp(sentence)\n",
    "            # Entity recognition\n",
    "            entity = [entity.text for entity in s_analysed.ents]\n",
    "            for words in entity:\n",
    "                modified_words = words.replace(\" \", \"_\")\n",
    "                sentence = sentence.replace(words, modified_words)        \n",
    "        s_analysed = nlp(sentence.replace(\"’s\", \"\")) # removal of ’s before tokenisation \n",
    "        for token in s_analysed:\n",
    "            if (token.is_stop == False) and (token.is_punct == False) and (token.like_url == False):\n",
    "                s_list.append(token.lemma_.lower())\n",
    "    return s_list\n",
    "\n",
    "def verb(text):\n",
    "    '''\n",
    "    return the strings with the verbs only\n",
    "    '''\n",
    "    return ' '.join([token.lemma_ \n",
    "             for token in nlp(text) \n",
    "             if token.pos_ in {'VERB'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['cleaned'] = data.headline.apply(preprocessing, entity_rec = True)\n",
    "#data['verb'] = data.headline.apply(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 28619 article headlines\n",
      "Presence of NAN False\n",
      "0    14985\n",
      "1    13634\n",
      "Name: is_sarcastic, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc2ac61f470>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.drop(\"article_link\", axis = 1, inplace = True)\n",
    "\n",
    "data['Set'] = np.random.choice(\n",
    "    ['train', 'valid', 'test'],\n",
    "    data.shape[0],\n",
    "    p=[0.7, 0.15, 0.15]\n",
    ")\n",
    "\n",
    "print(\"There are\", data.shape[0],'article headlines')\n",
    "print(\"Presence of NAN\" , data.isna().values.any())\n",
    "print(data.is_sarcastic.value_counts())\n",
    "data.is_sarcastic.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>verb</th>\n",
       "      <th>Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientist hair loss</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>0</td>\n",
       "      <td>totally nail fall short gender racial equality</td>\n",
       "      <td>nail fall</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>0</td>\n",
       "      <td>eat veggie deliciously different recipe</td>\n",
       "      <td>eat</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevent liar get work</td>\n",
       "      <td>prevent get work</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>1</td>\n",
       "      <td>mother come pretty close word streaming correctly</td>\n",
       "      <td>come use</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           headline  \\\n",
       "0           0  thirtysomething scientists unveil doomsday clo...   \n",
       "1           1  dem rep. totally nails why congress is falling...   \n",
       "2           2  eat your veggies: 9 deliciously different recipes   \n",
       "3           3  inclement weather prevents liar from getting t...   \n",
       "4           4  mother comes pretty close to using word 'strea...   \n",
       "\n",
       "   is_sarcastic                                            cleaned  \\\n",
       "0             1                thirtysomething scientist hair loss   \n",
       "1             0     totally nail fall short gender racial equality   \n",
       "2             0            eat veggie deliciously different recipe   \n",
       "3             1            inclement weather prevent liar get work   \n",
       "4             1  mother come pretty close word streaming correctly   \n",
       "\n",
       "               verb    Set  \n",
       "0               NaN  train  \n",
       "1         nail fall  train  \n",
       "2               eat  train  \n",
       "3  prevent get work  train  \n",
       "4          come use   test  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcasm = data.loc[data[\"is_sarcastic\"] == 1,[\"headline\",\"cleaned\",\"verb\"]]\n",
    "normal = data.loc[data[\"is_sarcastic\"] == 0,[\"headline\",\"cleaned\",\"verb\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research Question(s) (2 pts)\n",
    "\n",
    "Describe what question you are investigating with the data (max. 100 words)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis (6 pts)\n",
    "\n",
    "Apply at least one version of *each* of the following analysis methods to the data set (justify your choices):\n",
    "1. Topic modeling with LDA (3 pts). Justify your choice of number of topics!\n",
    "2. Dense word embeddings ***or*** document embeddings: visualize these and show a clustering (3 pts) \n",
    "\n",
    "Your analysis needs to be run on the training data only! You can use the dev set for tuning.\n",
    "\n",
    "### Other methods\n",
    "If appropriate for your problem, feel free to explore other methods, as long as they do not require additional libraries (**up to 2 bonus points**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcasm Words\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>area</td>\n",
       "      <td>493</td>\n",
       "      <td>4.319886</td>\n",
       "      <td>356.429390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>woman</td>\n",
       "      <td>487</td>\n",
       "      <td>4.346609</td>\n",
       "      <td>362.423936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>report</td>\n",
       "      <td>580</td>\n",
       "      <td>4.157367</td>\n",
       "      <td>433.993163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>new</td>\n",
       "      <td>1011</td>\n",
       "      <td>3.616648</td>\n",
       "      <td>797.145340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>man</td>\n",
       "      <td>1324</td>\n",
       "      <td>3.338043</td>\n",
       "      <td>914.863170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word    tf       idf       tfidf\n",
       "2     area   493  4.319886  356.429390\n",
       "44   woman   487  4.346609  362.423936\n",
       "30  report   580  4.157367  433.993163\n",
       "24     new  1011  3.616648  797.145340\n",
       "21     man  1324  3.338043  914.863170"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                             min_df=0.01, #df stands for document frequency\n",
    "                             max_df=0.6, \n",
    "                             stop_words='english')\n",
    "X = tfidf_vectorizer.fit_transform(sarcasm.cleaned.dropna().tolist())\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', min_df=0.01, max_df=0.6, stop_words='english')\n",
    "\n",
    "X2 = vectorizer.fit_transform(sarcasm.cleaned.dropna().tolist())\n",
    "\n",
    "df = pd.DataFrame(data={'word': vectorizer.get_feature_names(), \n",
    "                        'tf': X2.sum(axis=0).A1, \n",
    "                        'idf': tfidf_vectorizer.idf_,\n",
    "                        'tfidf': X.sum(axis=0).A1\n",
    "                       })\n",
    "print(\"Sarcasm Words\")\n",
    "df = df.sort_values(['tfidf', 'tf', 'idf'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>make</td>\n",
       "      <td>460</td>\n",
       "      <td>4.492379</td>\n",
       "      <td>364.375783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>woman</td>\n",
       "      <td>474</td>\n",
       "      <td>4.479307</td>\n",
       "      <td>374.177205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>say</td>\n",
       "      <td>546</td>\n",
       "      <td>4.323303</td>\n",
       "      <td>414.263847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>new</td>\n",
       "      <td>686</td>\n",
       "      <td>4.098679</td>\n",
       "      <td>533.928581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>trump</td>\n",
       "      <td>1446</td>\n",
       "      <td>3.343863</td>\n",
       "      <td>996.840361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word    tf       idf       tfidf\n",
       "19   make   460  4.492379  364.375783\n",
       "42  woman   474  4.479307  374.177205\n",
       "28    say   546  4.323303  414.263847\n",
       "22    new   686  4.098679  533.928581\n",
       "35  trump  1446  3.343863  996.840361"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                             min_df=0.01, \n",
    "                             max_df=0.6, \n",
    "                             stop_words='english')\n",
    "X = tfidf_vectorizer.fit_transform(normal.cleaned.dropna().tolist()) #I use the title to get the topic\n",
    "vectorizer = CountVectorizer(analyzer='word', min_df=0.01, max_df=0.6, stop_words='english')\n",
    "\n",
    "X2 = vectorizer.fit_transform(normal.cleaned.dropna().tolist())\n",
    "\n",
    "df = pd.DataFrame(data={'word': vectorizer.get_feature_names(), \n",
    "                        'tf': X2.sum(axis=0).A1, \n",
    "                        'idf': tfidf_vectorizer.idf_,\n",
    "                        'tfidf': X.sum(axis=0).A1\n",
    "                       })\n",
    "print(\"Normal\")\n",
    "df = df.sort_values(['tfidf', 'tf', 'idf'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "\n",
    "smoothing = 0.001\n",
    "counts = defaultdict(lambda: defaultdict(lambda: smoothing))\n",
    "sarcasm[\"headline\"] = sarcasm[\"headline\"].astype(str)\n",
    "corpus = [line.strip().split() for line in sarcasm.headline.tolist()]\n",
    "\n",
    "#collect and count\n",
    "for sentence in corpus:\n",
    "    #include start and stop in the sentence\n",
    "    tokens = ['*', '*','*','*','*'] + sentence + ['STOP']\n",
    "    for u, v, w, x, y, z in nltk.ngrams(tokens, 6): #we create the ngrams from the sentence and we count them\n",
    "        counts[u, v, w, x, y][z] += 1\n",
    "\n",
    "def logP(u, v, w):\n",
    "    #(u,v,w) => P(w) preceeded by u,v we compute the log proba. to avoid numbers that are too small to divivde byand get infinty\n",
    "    return np.log(counts[(u, v)][w]) - np.log(sum(counts[(u, v)].values()))\n",
    "\n",
    "def sentence_logP(S):\n",
    "    #score a sentence in log likelihood with chain rule (product becomes sum with log) S: lits of strings\n",
    "    tokens = ['*', '*'] + S + ['STOP']\n",
    "    return sum([logP(u, v, w) for u, v, w in nltk.ngrams(tokens, 3)])\n",
    "    #for each sentence we eaxtract the trigrams and we compute their proba, and then sum it so we get the log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_word(u, v, w, x, y):\n",
    "    #sample a word v based on the history (u,v)\n",
    "    keys, values = zip(*counts[(u, v, w, x, y)].items()) #items returns a litsof tuples keys and values\n",
    "    #zip takes any number of arguments and zips them together \n",
    "    values = np.array(values)\n",
    "    values /= values.sum() # create probability distro\n",
    "    sample = np.random.multinomial(1, values) # pick one position (returns a position)\n",
    "    return keys[np.argmax(sample)]\n",
    "\n",
    "def generate2(initial=[]):\n",
    "    result = ['*', '*','*','*','*'] + initial\n",
    "    next_word = sample_next_word(result[-5], result[-4], result[-3],result[-2], result[-1])\n",
    "    result.append(next_word)\n",
    "    while next_word != 'STOP':\n",
    "        next_word = sample_next_word(result[-5], result[-4], result[-3],result[-2], result[-1])\n",
    "        result.append(next_word)\n",
    "\n",
    "    return ' '.join(result[2:-1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * john bolton insists iran likely harboring dangerous terrorist osama bin laden\n"
     ]
    }
   ],
   "source": [
    "print(generate2(['john']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * john edwards pays $30 to register edwards2016.com just in case\n"
     ]
    }
   ],
   "source": [
    "print(generate2(['john']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * john kelly resigns in last-ditch effort to save his and trump's friendship\n"
     ]
    }
   ],
   "source": [
    "print(generate2(['john']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * politicians ignoring the dangers of jowl implants\n"
     ]
    }
   ],
   "source": [
    "print(generate2(['politicians']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * universe feels zero connection to guy tripping on mushrooms\n"
     ]
    }
   ],
   "source": [
    "print(generate2([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#checking if the generated headlines is in the training data\n",
    "print(sarcasm[\"headline\"].str.contains(\"universe feels zero connection to guy tripping on mushrooms\").any())\n",
    "print(sarcasm[\"headline\"].str.contains(\"politicians ignoring the dangers of jowl implants\").any())\n",
    "print(sarcasm[\"headline\"].str.contains(\"john bolton insists iran likely harboring dangerous terrorist osama bin laden\").any())\n",
    "print(sarcasm[\"headline\"].str.contains(\"john edwards pays $30 to register edwards2016.com just in case\").any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying with 2grams because with 6grams its too specific and the language model just spits out headlines from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "for sentence in corpus:\n",
    "    tokens = ['*', '*'] + sentence + ['STOP']\n",
    "    for u, v, w in nltk.ngrams(tokens, 3):\n",
    "        counts[(u, v)][w] += 1\n",
    "\n",
    "def logP(u, v, w):\n",
    "    return np.log(counts[(u, v)][w]) - np.log(sum(counts[(u, v)].values()))\n",
    "\n",
    "def sentence_logP(S):\n",
    "    tokens = ['*', '*'] + S + ['STOP']\n",
    "    return sum([logP(u, v, w) for u, v, w in nltk.ngrams(tokens, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_word(u, v):\n",
    "    keys, values = zip(*counts[(u, v)].items())\n",
    "    values = np.array(values)\n",
    "    values /= values.sum() # create probability distro\n",
    "    sample = np.random.multinomial(1, values) # pick one position\n",
    "    return keys[np.argmax(sample)]\n",
    "\n",
    "def generate():\n",
    "    result = ['*', '*']\n",
    "    next_word = sample_next_word(result[-2], result[-1])\n",
    "    result.append(next_word)\n",
    "    while next_word != 'STOP':\n",
    "        next_word = sample_next_word(result[-2], result[-1])\n",
    "        result.append(next_word)\n",
    "\n",
    "    return ' '.join(result[2:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dog can't believe he left consulate\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deceased souls backed up at tall building thinking about, you know'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sighing trump sexual assault has cooled down'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'increasingly desperate advertisers settle for more attainable 35-to-44-year-old demographic'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'matt lauer returns to stage for humans'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(sarcasm[\"headline\"].str.contains(\"dog can't believe he left consulate\").any())\n",
    "print(sarcasm[\"headline\"].str.contains(\"deceased souls backed up at tall building thinking about, you know\").any())\n",
    "print(sarcasm[\"headline\"].str.contains(\"sighing trump sexual assault has cooled down\").any())\n",
    "print(sarcasm[\"headline\"].str.contains(\"matt lauer returns to stage for humans\").any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LDA topic model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dictionary\n",
      "Dictionary(74 unique tokens: ['get', 'work', 'come', 'run', 'area']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaMulticore, TfidfModel, CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "import time # to know how long training took\n",
    "import multiprocessing # to speed things up by parallelizing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get dictionary\n",
    "instances = sarcasm.cleaned.dropna().apply(str.split)\n",
    "print(\"creating dictionary\", flush=True)\n",
    "# read in instances and create Dictionary object w information about frequencies etc. \n",
    "dictionary = Dictionary(instances)\n",
    "# get rid of words that are too rare or too frequent\n",
    "dictionary.filter_extremes(no_below=100, no_above=0.3) #we restrict ousrlef with words that occur more than 50 times, and words that occur below 30% of the documents. \n",
    "print(dictionary, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             [thirtysomething, scientist, hair, loss]\n",
       "3       [inclement, weather, prevent, liar, get, work]\n",
       "4    [mother, come, pretty, close, word, streaming,...\n",
       "7    [warm, donation, nearly, cost, fail, balloon, ...\n",
       "8         [shadow, government, get, large, meet, room]\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translating corpus to IDs\n",
      "tf-idf transformation\n"
     ]
    }
   ],
   "source": [
    "#replace words by their numerical IDs and their frequency\n",
    "print(\"translating corpus to IDs\", flush=True)\n",
    "ldacorpus = [dictionary.doc2bow(text) for text in instances] #replace each words by its id in our vocabulary, list of list of integers\n",
    "# learn TFIDF values from corpus\n",
    "print(\"tf-idf transformation\", flush=True)\n",
    "tfidfmodel = TfidfModel(ldacorpus)\n",
    "# transform raw frequencies into TFIDF\n",
    "model_corpus = tfidfmodel[ldacorpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679.75\n",
      "fitting model\n",
      "done in 3.3294999599456787\n"
     ]
    }
   ],
   "source": [
    "num_topics = 11\n",
    "\n",
    "# find chunksize to make about 200 updates\n",
    "num_passes = 10\n",
    "chunk_size = len(model_corpus) * num_passes/200\n",
    "print(chunk_size)\n",
    "\n",
    "start = time.time()\n",
    "print(\"fitting model\", flush=True)\n",
    "model = LdaMulticore(num_topics=num_topics, # number of topics\n",
    "                     corpus=model_corpus, # what to train on \n",
    "                     id2word=dictionary, # mapping from IDs to words\n",
    "                     workers=min(10, multiprocessing.cpu_count()-1), # choose 10 cores, or whatever computer has\n",
    "                     passes=num_passes, # make this many passes over data\n",
    "                     chunksize=chunk_size, # update after this many instances\n",
    "                     alpha=0.5 #controls how many topics we expcet to see in a document. if alpha is big we expect to see all topics in one document, large alpha is 100\n",
    "                    )\n",
    "    \n",
    "print(\"done in {}\".format(time.time()-start), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcasm Topics\n",
      "1 \"friend\", \"call\", \"kill\", \"reveal\", \"ask\"\n",
      "2 \"nation\", \"man\", \"come\", \"find\", \"take\"\n",
      "3 \"man\", \"think\", \"area\", \"run\", \"new\"\n",
      "4 \"look\", \"man\", \"people\", \"little\", \"life\"\n",
      "5 \"get\", \"give\", \"spend\", \"trump\", \"try\"\n",
      "6 \"new\", \"year\", \"thing\", \"guy\", \"local\"\n",
      "7 \"go\", \"family\", \"death\", \"school\", \"want\"\n",
      "8 \"know\", \"way\", \"watch\", \"time\", \"mom\"\n",
      "9 \"introduce\", \"plan\", \"man\", \"report\", \"feel\"\n",
      "10 \"old\", \"woman\", \"study\", \"world\", \"couple\"\n",
      "11 \"day\", \"self\", \"child\", \"report\", \"announce\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# get the topic descritions\n",
    "topic_sep = re.compile(r\"0\\.[0-9]{3}\\*\") # getting rid of useless formatting\n",
    "# extract a list of tuples with topic number and descriptors from the model\n",
    "model_topics = [(topic_no, re.sub(topic_sep, '', model_topic).split(' + ')) for topic_no, model_topic in\n",
    "                model.print_topics(num_topics=num_topics, num_words=5)]\n",
    "\n",
    "print(\"Sarcasm Topics\")\n",
    "descriptors = []\n",
    "for i, m in model_topics:\n",
    "    print(i+1, \", \".join(m[:5]))\n",
    "    descriptors.append(\", \".join(m[:2]).replace('\"', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non sarcasm LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dictionary\n",
      "Dictionary(85 unique tokens: ['way', 'know', 'parent', 'tell', 'leave']...)\n"
     ]
    }
   ],
   "source": [
    "# get dictionary\n",
    "normal['verb'] = normal.cleaned.apply(str)\n",
    "# run on 50000 instances\n",
    "instances = normal.cleaned.dropna().apply(str.split)\n",
    "print(\"creating dictionary\", flush=True)\n",
    "# read in instances and create Dictionary object w information about frequencies etc. \n",
    "dictionary = Dictionary(instances)\n",
    "# get rid of words that are too rare or too frequent\n",
    "dictionary.filter_extremes(no_below=100, no_above=0.3) #we restrict ousrlef with words that occur more than 50 times, and words that occur below 30% of the documents. \n",
    "print(dictionary, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translating corpus to IDs\n",
      "tf-idf transformation\n"
     ]
    }
   ],
   "source": [
    "#replace words by their numerical IDs and their frequency\n",
    "print(\"translating corpus to IDs\", flush=True)\n",
    "ldacorpus = [dictionary.doc2bow(text) for text in instances] #replace each words by its id in our vocabulary, list of list of integers\n",
    "# learn TFIDF values from corpus\n",
    "print(\"tf-idf transformation\", flush=True)\n",
    "tfidfmodel = TfidfModel(ldacorpus)\n",
    "# transform raw frequencies into TFIDF\n",
    "model_corpus = tfidfmodel[ldacorpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743.0\n",
      "fitting model\n",
      "done in 4.317033290863037\n"
     ]
    }
   ],
   "source": [
    "num_topics = 11\n",
    "\n",
    "# find chunksize to make about 200 updates\n",
    "num_passes = 10\n",
    "chunk_size = len(model_corpus) * num_passes/200\n",
    "print(chunk_size)\n",
    "\n",
    "start = time.time()\n",
    "print(\"fitting model\", flush=True)\n",
    "model = LdaMulticore(num_topics=num_topics, # number of topics\n",
    "                     corpus=model_corpus, # what to train on \n",
    "                     id2word=dictionary, # mapping from IDs to words\n",
    "                     workers=min(10, multiprocessing.cpu_count()-1), # choose 10 cores, or whatever computer has\n",
    "                     passes=num_passes, # make this many passes over data\n",
    "                     chunksize=chunk_size, # update after this many instances\n",
    "                     alpha=0.5 #controls how many topics we expcet to see in a document. if alpha is big we expect to see all topics in one document, large alpha is 100\n",
    "                    )\n",
    "    \n",
    "print(\"done in {}\".format(time.time()-start), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOrmal headlines Topics\n",
      "1 \"right\", \"make\", \"war\", \"take\", \"think\"\n",
      "2 \"year\", \"thing\", \"day\", \"bad\", \"real\"\n",
      "3 \"good\", \"watch\", \"school\", \"fire\", \"let\"\n",
      "4 \"trump\", \"say\", \"plan\", \"know\", \"fight\"\n",
      "5 \"people\", \"talk\", \"tell\", \"want\", \"climate\"\n",
      "6 \"new\", \"attack\", \"go\", \"care\", \"health\"\n",
      "7 \"get\", \"photo\", \"campaign\", \"leave\", \"time\"\n",
      "8 \"video\", \"death\", \"live\", \"student\", \"family\"\n",
      "9 \"old\", \"gun\", \"police\", \"work\", \"star\"\n",
      "10 \"woman\", \"man\", \"help\", \"american\", \"week\"\n",
      "11 \"win\", \"come\", \"election\", \"world\", \"find\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# get the topic descritions\n",
    "topic_sep = re.compile(r\"0\\.[0-9]{3}\\*\") # getting rid of useless formatting\n",
    "# extract a list of tuples with topic number and descriptors from the model\n",
    "model_topics = [(topic_no, re.sub(topic_sep, '', model_topic).split(' + ')) for topic_no, model_topic in\n",
    "                model.print_topics(num_topics=num_topics, num_words=5)]\n",
    "\n",
    "print(\"NOrmal headlines Topics\")\n",
    "descriptors = []\n",
    "for i, m in model_topics:\n",
    "    print(i+1, \", \".join(m[:5]))\n",
    "    descriptors.append(\", \".join(m[:2]).replace('\"', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Result : sarcasm doesn't differentiate itself by the topic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction (15 pts)\n",
    "\n",
    "\n",
    "### 4.1 Classification (9 pts)\n",
    "Build a predictive model of the target label and use appropriate performance metrics. Your predictive analysis needs to involve **all** of the following, summarized in a table:\n",
    "\n",
    "1. a most-frequent-label baseline (1 point)\n",
    "2. a `LogisticRegression()` baseline with default parameters and 2-6 gram character TFIDF features (1 pt)\n",
    "3. the performance of **at least** two more predictive model architecture (2 pts each), including description/justification of the optmization steps taken (2 pts).\n",
    "4. two bootstrap sampling significance tests of the performance difference between your best model and each of the two baselines (1 pts)\n",
    "\n",
    "NB: Do make sure that the optimization steps are done on the development split and do *not* include the test split! Training on the test set will be graded 0!\n",
    "\n",
    "### 4.1 Structured Prediction (6pts)\n",
    "Run the Structured Prediction model as-is on your sequence prediction task, and note the performance as baseline (1 pt).\n",
    "Change the features to improve performance (2 pts).\n",
    "Run a suitable neural net implementation (in `keras`) on the data and compare the best performance to the other two models (4 pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      1.00      0.69      2259\n",
      "           1       0.00      0.00      0.00      2077\n",
      "\n",
      "    accuracy                           0.52      4336\n",
      "   macro avg       0.26      0.50      0.34      4336\n",
      "weighted avg       0.27      0.52      0.36      4336\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "X_train = data.loc[data.Set =='train', [\"cleaned\"]]\n",
    "y_train = data.loc[data.Set =='train', [\"is_sarcastic\"]]\n",
    "\n",
    "X_dev = data.loc[data.Set =='valid', [\"cleaned\"]]\n",
    "y_dev =data.loc[data.Set =='valid', [\"is_sarcastic\"]]\n",
    "\n",
    "most_frequent = DummyClassifier(strategy='most_frequent')\n",
    "most_frequent.fit(X_train, y_train)\n",
    "dumb_predictions = most_frequent.predict(X_dev)\n",
    "\n",
    "print(classification_report(y_dev, dumb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "After pruning, no terms remain. Try a lower min_df or a higher max_df.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-f734fb39f114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                              analyzer='word')\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \"\"\"\n\u001b[1;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                                                        \u001b[0mmin_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m                                                        max_features)\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_limit_features\u001b[0;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0mkept_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkept_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             raise ValueError(\"After pruning, no terms remain. Try a lower\"\n\u001b[0m\u001b[1;32m   1110\u001b[0m                              \" min_df or a higher max_df.\")\n\u001b[1;32m   1111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkept_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremoved_terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: After pruning, no terms remain. Try a lower min_df or a higher max_df."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), \n",
    "                             min_df=0.1, \n",
    "                             max_df=0.9, \n",
    "                             analyzer='word')\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'thirtysomething scientist hair loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1527\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'thirtysomething scientist hair loss'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=-1, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'thirtysomething scientist hair loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1527\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'thirtysomething scientist hair loss'"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-23e3c158e120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                          class_weight='balanced')\n\u001b[1;32m     10\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'classifier_balanced.fit(X_train, y_train)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpredictions_balanced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_balanced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_balanced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mclass\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \"\"\"\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs')\n",
    "%time classifier.fit(X_train, y_train)\n",
    "print(classifier)\n",
    "\n",
    "classifier_balanced = LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', \n",
    "                                         class_weight='balanced')\n",
    "\n",
    "%time classifier_balanced.fit(X_train, y_train)\n",
    "predictions_balanced = classifier_balanced.predict(X_dev)\n",
    "\n",
    "print(classification_report(y_dev, predictions_balanced))\n",
    "\n",
    "predictions_balanced_test = classifier_balanced.predict(X_test)\n",
    "print(classification_report(y_test, predictions_balanced_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizations (3 pts)\n",
    "\n",
    "Provide at least 3 visualizations of your work above. These can be in the respective sections. Use labels and legends. Be creative. Just please do not use word clouds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
